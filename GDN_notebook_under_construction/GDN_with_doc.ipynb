{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dHdJIYKUmOxG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyCa62G9fbXx"
   },
   "source": [
    "## **Contents**\n",
    "\n",
    "1.   [Why Graph Neural Networks](#whygnn)?\n",
    "2.   [Data Pre-Processing](#data_preprocessing)\n",
    "3.   [Utilities](#util)<br>\n",
    "        \n",
    "4.   [TimeDataset (Preparing to Forecast)](#TimeDataset)\n",
    "5.   [Graph Attention-Based Forecasting](#graph_layer)\n",
    "6.   [Graph Structure Learning + GDN](#gdn)\n",
    "7.   [Main](#driver)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2SmIBKbnYJq"
   },
   "source": [
    "### Why Graph Neural Networks?\n",
    "\n",
    "<a id = \"whygnn\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV4Z4V3tm-AF"
   },
   "source": [
    "\n",
    "1.   Given high-dimensional time series data (e.g., sensor data), how can we detect anomalous events ?, events such as system faults\n",
    "and attacks? More challenging, how can we do this in a way that captures complex inter-sensor relationships, and detects and explains anomalies which deviate from these relationships?\n",
    "2.   Capturing only the linear relationships is insufficient for complex, highly nonlinear relationships in many real-world settings. Data from these sensors can be related in complex, nonlinear ways. To learn representations for nonlinear high-dimensional time series and predict time series data, deep learning based time series methods have attracted interest in recent years.\n",
    "\n",
    "3.  In recent years, graph neural networks (GNNs) have emerged as successful approaches for modelling complex patterns in graph-structured data. In\n",
    "general, GNNs assume that the state of a node is influenced by the states of its neighbors. \n",
    "\n",
    "4. GNNs use the same model parameters to model the behavior of each node, and hence face limitations in representing very different behaviors of different sensors. Moreover, GNNs typically require the graph structure as an input, whereas the graph structure is initially unknown in many cases, and needs to be learned from data.\n",
    "\n",
    "5. GNNs are highly scalable as they use one model, and the same set of parameters to get the embeddings for all nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2LE1WGVCCYn"
   },
   "source": [
    "### Data Pre-Processing(Example using WADI data set, depending on the problem, it can be modified.)\n",
    "\n",
    "<a id = \"data_preprocessing\"> </a>\n",
    "\n",
    "\n",
    "1. We can modify the paths to training and testing data. \n",
    "2. Preprocessing involves missing data imputation with the mean values, normalizing the train/test data with MinMax Scaler. \n",
    "3. Downsample the data by 10 units(pick 1 timestep that represent 10 timesteps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EBGSYXiMxAzT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling Train Data\n",
      "Shape of the data without Downsample: (784571, 126), Labels: (784571,)\n",
      "Shape of the data with Downsample: (78457, 126), Labels: (78457,)\n",
      "Downsampling Test Data\n",
      "Shape of the data without Downsample: (172803, 126), Labels: (172803,)\n",
      "Shape of the data with Downsample: (17280, 126), Labels: (17280,)\n"
     ]
    }
   ],
   "source": [
    "def norm(train, test):\n",
    "    normalizer = MinMaxScaler(feature_range=(0, 1)).fit(train) # scale training data to [0,1] range\n",
    "    train_ret = normalizer.transform(train)\n",
    "    test_ret = normalizer.transform(test)\n",
    "\n",
    "    return train_ret, test_ret\n",
    "\n",
    "\n",
    "def downsample(data, labels, down_len):\n",
    "    np_data = np.array(data)\n",
    "    np_labels = np.array(labels)\n",
    "    print(\"Shape of the data without Downsample: {}, Labels: {}\".format(np_data.shape, np_labels.shape))\n",
    "    orig_len, col_num = np_data.shape\n",
    "    down_time_len = orig_len // down_len\n",
    "    np_data = np_data.transpose()\n",
    "    d_data = np_data[:, :down_time_len*down_len].reshape(col_num, -1, down_len)\n",
    "    d_data = np.median(d_data, axis=2).reshape(col_num, -1)\n",
    "\n",
    "    d_labels = np_labels[:down_time_len*down_len].reshape(-1, down_len)\n",
    "    # if exist anomalies, then this sample is abnormal\n",
    "    d_labels = np.round(np.max(d_labels, axis=1))\n",
    "\n",
    "    d_data = d_data.transpose()\n",
    "    print(\"Shape of the data with Downsample: {}, Labels: {}\".format(d_data.shape, d_labels.shape))\n",
    "    return d_data.tolist(), d_labels.tolist()\n",
    "\n",
    "\n",
    "def pre_process(train_path, test_path, list_save_path, train_save_path, test_save_path):\n",
    "\n",
    "    train = pd.read_csv(train_path, index_col=0)\n",
    "    test = pd.read_csv(test_path, index_col=0)\n",
    "    train = train.iloc[:, 3:]\n",
    "    test = test.iloc[:, 3:]\n",
    "    train = train.fillna(train.mean())\n",
    "    test = test.fillna(test.mean())\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "\n",
    "    # trim column names\n",
    "    train = train.rename(columns=lambda x: x.strip())\n",
    "    test = test.rename(columns=lambda x: x.strip())\n",
    "\n",
    "    train_labels = np.zeros(len(train))\n",
    "    test_labels = test.attack\n",
    "\n",
    "\n",
    "    test = test.drop(columns=['attack'])\n",
    "    cols = train.columns\n",
    "    train.columns = cols\n",
    "    test.columns = cols\n",
    "\n",
    "    x_train, x_test = norm(train.values, test.values)\n",
    "    \n",
    "    print(\"Downsampling Train Data\")\n",
    "    d_train_x, d_train_labels = downsample(x_train, train_labels, 10)\n",
    "    print(\"Downsampling Test Data\")\n",
    "    d_test_x, d_test_labels = downsample(x_test, test_labels, 10)\n",
    "\n",
    "    train_df = pd.DataFrame(d_train_x, columns = train.columns)\n",
    "    test_df = pd.DataFrame(d_test_x, columns = test.columns)\n",
    "\n",
    "\n",
    "    test_df['attack'] = d_test_labels\n",
    "    train_df['attack'] = d_train_labels\n",
    "\n",
    "    train_df = train_df.iloc[2160:]\n",
    "\n",
    "    train_df.to_csv(train_save_path)\n",
    "    test_df.to_csv(test_save_path)\n",
    "\n",
    "    f = open(list_save_path, 'w')\n",
    "    for col in train.columns:\n",
    "        f.write(col+'\\n')\n",
    "    f.close()\n",
    "\n",
    "train_path = \"C:/Users/harsh/OneDrive/Desktop/notebooks/WADI_14days.csv\"\n",
    "test_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/WADI_attackdata_labelled.csv'\n",
    "list_save_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/list.txt'\n",
    "\n",
    "train_save_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/train.csv'\n",
    "test_save_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/test.csv'\n",
    "\n",
    "\n",
    "pre_process(train_path, test_path, list_save_path, train_save_path, test_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnbNMHzFqQsc"
   },
   "source": [
    "### Utilities\n",
    "<a id = 'util'></a>\n",
    "\n",
    "Functions include helpers (assigning data to a device, **ex**: cuda or cpu), calculating scores(F1 etc) and statistics (median, IQR etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QUe5yQ4186-f"
   },
   "outputs": [],
   "source": [
    "def get_feature_map(feature_list_path):\n",
    "    \"\"\"\n",
    "    Params: feature_list_path\n",
    "                list.txt file with all the features(sensor) names.\n",
    "            \n",
    "    return: \n",
    "            List of feature names, simply, converts list.txt file to actual list.  \n",
    "    \"\"\"\n",
    "    feature_file = open(feature_list_path, 'r')\n",
    "    feature_list = []\n",
    "    for ft in feature_file:\n",
    "        feature_list.append(ft.strip())\n",
    "    \n",
    "    return feature_list\n",
    "\n",
    "\n",
    "def get_fc_graph_struc(feature_list_path):\n",
    "    \"\"\"\n",
    "    Params: feature_list_path\n",
    "                list.txt file with all the features(sensor) names.\n",
    "            \n",
    "    return: \n",
    "            Dictionary with Key being the feature name, value being  the rest of the features. \n",
    "            Simply put, a dense graph without self loops. \n",
    "    \"\"\"\n",
    "    feature_file = open(feature_list_path, 'r')\n",
    "    struc_map = {}\n",
    "    feature_list = []\n",
    "    for feature in feature_file:\n",
    "        feature_list.append(feature.strip())\n",
    "\n",
    "    for feature in feature_list:\n",
    "        if feature not in struc_map:\n",
    "            struc_map[feature] = []\n",
    "\n",
    "        for other_feature in feature_list:\n",
    "            if other_feature!=feature:\n",
    "                struc_map[feature].append(other_feature)\n",
    "    \n",
    "    return struc_map\n",
    "\n",
    "\n",
    "def construct_data(data, feature_map, labels=0):\n",
    "    res = []\n",
    "\n",
    "    for feature in feature_map:\n",
    "        if feature in data.columns:\n",
    "            res.append(data.loc[:, feature].values.tolist())\n",
    "        else:\n",
    "            print(feature, 'not exist in data')\n",
    "    # append labels as last\n",
    "    sample_n = len(res[0])\n",
    "\n",
    "    if type(labels) == int:\n",
    "        res.append([labels]*sample_n)\n",
    "    elif len(labels) == sample_n:\n",
    "        res.append(labels)\n",
    "\n",
    "    return res\n",
    "\n",
    "def build_loc_net(struc, all_features, feature_map=[]):\n",
    "\n",
    "    index_feature_map = feature_map\n",
    "    edge_indexes = [[],[]]\n",
    "    for node_name, node_list in struc.items():\n",
    "        if node_name not in all_features:\n",
    "            continue\n",
    "\n",
    "        if node_name not in index_feature_map:\n",
    "            index_feature_map.append(node_name)\n",
    "        \n",
    "        p_index = index_feature_map.index(node_name)\n",
    "        for child in node_list:\n",
    "            if child not in all_features:\n",
    "                continue\n",
    "\n",
    "            if child not in index_feature_map:\n",
    "                print(f'error: {child} not in index_feature_map')\n",
    "                #index_feature_map.append(child)\n",
    "\n",
    "            c_index = index_feature_map.index(child)\n",
    "            edge_indexes[0].append(c_index)\n",
    "            edge_indexes[1].append(p_index)\n",
    "\n",
    "    return edge_indexes\n",
    "\n",
    "def get_batch_edge_index(org_edge_index, batch_num, node_num):\n",
    "    # org_edge_index:(2, edge_num)\n",
    "    edge_index = org_edge_index.clone().detach()\n",
    "    edge_num = org_edge_index.shape[1]\n",
    "    batch_edge_index = edge_index.repeat(1,batch_num).contiguous()\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        batch_edge_index[:, i*edge_num:(i+1)*edge_num] += i*node_num\n",
    "\n",
    "    return batch_edge_index.long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pQygPsadsMh"
   },
   "source": [
    "### **TimeDataset (Preparing to Forecast)**\n",
    "\n",
    "<a id='TimeDataset'></a>\n",
    "\n",
    "1.   Thus, at time t, define the model input $x(t) \\in \\mathbb{R}^{N \\times W}$\n",
    "based on a sliding window of size w over the historical time\n",
    "series data (whether training or testing data).\n",
    "\n",
    "\n",
    ">    <center>$x_{t} = [s^{(t - w)}, s^{(t - w + 1)} .... s^{(t - 1)}]$</center>\n",
    "\n",
    "<center> The target output that the model needs to predict is the sensor data at the current time tick, i.e. $s^{(t)}$. </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GET51afCykEc"
   },
   "outputs": [],
   "source": [
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, raw_data, edge_index, mode, stride, window_len):\n",
    "        self.raw_data = raw_data\n",
    "        self.edge_index = edge_index\n",
    "        self.mode = mode\n",
    "        self.slide_win = window_len\n",
    "        self.slide_stride = stride\n",
    "\n",
    "        x_data = raw_data[:-1]\n",
    "        labels = raw_data[-1]\n",
    "        data = x_data\n",
    "        # to tensor\n",
    "        data = torch.tensor(data).double()\n",
    "        labels = torch.tensor(labels).double()\n",
    "\n",
    "        self.x, self.y, self.labels = self.process(data, labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "    def process(self, data, labels):\n",
    "        x_arr, y_arr = [], []\n",
    "        labels_arr = []\n",
    "        \n",
    "        is_train = self.mode == 'train'\n",
    "        node_num, total_time_len = data.shape\n",
    "        rang = range(self.slide_win, total_time_len, self.slide_stride) if is_train else range(self.slide_win, total_time_len)\n",
    "        \n",
    "        for i in rang:\n",
    "            ft = data[:, i-self.slide_win:i]\n",
    "            tar = data[:, i]\n",
    "            x_arr.append(ft)\n",
    "            y_arr.append(tar)\n",
    "            labels_arr.append(labels[i])\n",
    "\n",
    "        x = torch.stack(x_arr).contiguous()\n",
    "        y = torch.stack(y_arr).contiguous()\n",
    "\n",
    "        labels = torch.Tensor(labels_arr).contiguous()\n",
    "        \n",
    "        return x, y, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.x[idx].double()\n",
    "        y = self.y[idx].double()\n",
    "        edge_index = self.edge_index.long()\n",
    "        label = self.labels[idx].double()\n",
    "\n",
    "        return feature, y, label, edge_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvho8te46CHv"
   },
   "source": [
    "### **Graph Attention-Based Forecasting**\n",
    "\n",
    "\n",
    "<a id='graph_layer'></a>\n",
    "\n",
    "\n",
    "1.   To capture the relationships between\n",
    "sensors, a graph attention-based feature extractor is introduced to fuse a node’s information with its neighbors based on\n",
    "the learned graph structure. Feature extractor incorporates the sensor\n",
    "embedding vectors $v_i$\n",
    ", which characterize the different behaviors of different types of sensors. To do this, compute\n",
    "node i’s aggregated representation $z_i$ as follows:\n",
    "\n",
    "<center>$z^{(t)}_{i} = ReLU(\\alpha_{i, i}\\textbf{W}x^{(t)}_i + \\sum_{j \\in N(i)} \\alpha_{i, j}\\textbf{W}x^{(t)}_j)$</center>\n",
    "\n",
    "\n",
    "<center>where $x^{(t)}_i \\in \\mathbb{R}^{w}$ is node i's input feature\n",
    "$N(i) = {j | A_{ji} > 0}$ is the set of neighbors of node i obtained from\n",
    "the learned adjacency matrix A, $W \\in \\mathbb{R}^{d \\times w}$ is a trainable\n",
    "weight matrix which applies a shared linear transformation to every node, and the attention coefficients $\\alpha_{i, j}$ are computed as:</center>\n",
    "\n",
    "<center>$g^{(t)}_{i} = v_{i} \\oplus \\textbf{W}x^{(t)}_{i}$</center>\n",
    "\n",
    "<center>$\\pi(i, j) = LeakyReLU(a^{T} (g^{(t)}_{i} \\oplus g^{(t)}_{j}))$</center>\n",
    "<center>$\\alpha(i, j) = SoftMax(\\pi(i, j))$</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Graph Structure Learning + GDN\n",
    "<a id = \"gdn\"></a>\n",
    "\n",
    "1.   A major goal of this framework is to learn the relationships\n",
    "between sensors in the form of a graph structure. To do this,\n",
    "a directed graph is used, whose nodes represent sensors, and whose edges represent dependency relationships\n",
    "between them.\n",
    "\n",
    "2. An edge from one sensor to another indicates\n",
    "that the first sensor is used for modelling the behavior of the\n",
    "second sensor. **A directed graph is used because the dependency patterns between sensors need not be symmetric.**\n",
    "\n",
    "3. A flexible framework is applied either to:<br>\n",
    "    3.1 The usual case where we have no prior information about the graph structure.<br>\n",
    "    \n",
    "    3.2 The case where we have some prior information about which edges are plausible (e.g. the sensor system may be divided into parts, where sensors in different parts have minimal interaction).<br>\n",
    "\n",
    "\n",
    "4. This prior information can be flexibly represented as a set\n",
    "of candidate relations $C_i$ for each sensor i, i.e. the sensors\n",
    "it could be dependent on:\n",
    "\n",
    "<center>{$C_i \\subset \\{1,3, 8, ... N\\}$ \\ ${i}$, no self loop.}</center>\n",
    "\n",
    "\n",
    "5. In the case without prior information, the candidate relations\n",
    "of sensor i is simply all sensors, other than itself.\n",
    "\n",
    "6.  The output of our algorithm is a set of $T_{test}$  binary labels\n",
    "indicating whether each test time tick is  = 1 an anomaly or not,\n",
    "i.e. $a(t) \\in \\{0, 1\\}$, where $a(t)$ indicates that time(t) is\n",
    "anomalous.\n",
    "\n",
    "7.  To select the dependencies of sensor i among these candidates, compute the similarity between node i’s embedding vector, and the embeddings of its candidates ${j \\in C_{i}}$\n",
    "\n",
    "That is, first compute $e_{ji}$, the normalized dot product between the embedding vectors of sensor i, and the candidate\n",
    "relation $j \\in C_{i}$\n",
    ". Then select the top k such normalized\n",
    "dot products: here **TopK** denotes the indices of top-k values among its input (i.e. the normalized dot products). **The value of k can be chosen by the user according to the desired sparsity level**. Next, a graph attention-based\n",
    "model is defined which makes use of this learned adjacency matrix A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KnV35iIlwVPD"
   },
   "outputs": [],
   "source": [
    "#Renamed GraphLayer to GATLayer\n",
    "class GATLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True, negative_slope=0.2, dropout=0, bias=True, inter_dim=-1,**kwargs):\n",
    "        super(GATLayer, self).__init__(aggr='add', **kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.node_dim = 0\n",
    "        self.__alpha__ = None\n",
    "        self.lin = Linear(in_channels, heads * out_channels, bias=False)\n",
    "\n",
    "        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads*out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        glorot(self.att_i)\n",
    "        glorot(self.att_j)\n",
    "        zeros(self.att_em_i)\n",
    "        zeros(self.att_em_j)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, embedding, return_attention_weights=False):\n",
    "        if torch.is_tensor(x):\n",
    "            x = self.lin(x)\n",
    "            x = (x, x)\n",
    "        else:\n",
    "            x = (self.lin(x[0]), self.lin(x[1]))\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x[1].size(self.node_dim))\n",
    "        out = self.propagate(edge_index, x=x, embedding=embedding, edges=edge_index, return_attention_weights=return_attention_weights)\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if return_attention_weights:\n",
    "            alpha, self.__alpha__ = self.__alpha__, None\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index_i, size_i, embedding, edges, return_attention_weights):\n",
    "        x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding_i, embedding_j = embedding[edge_index_i], embedding[edges[0]]\n",
    "            embedding_i = embedding_i.unsqueeze(1).repeat(1,self.heads,1)\n",
    "            embedding_j = embedding_j.unsqueeze(1).repeat(1,self.heads,1)\n",
    "\n",
    "            key_i = torch.cat((x_i, embedding_i), dim=-1)\n",
    "            key_j = torch.cat((x_j, embedding_j), dim=-1)\n",
    "\n",
    "        cat_att_i = torch.cat((self.att_i, self.att_em_i), dim=-1)\n",
    "        cat_att_j = torch.cat((self.att_j, self.att_em_j), dim=-1)\n",
    "        alpha = (key_i * cat_att_i).sum(-1) + (key_j * cat_att_j).sum(-1)\n",
    "        alpha = alpha.view(-1, self.heads, 1)\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, edge_index_i, num_nodes = size_i)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            self.__alpha__ = alpha\n",
    "\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return x_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, inter_dim=0, heads=1, node_num=100):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.gnn = GATLayer(in_channel, out_channel, inter_dim=inter_dim, heads=heads, concat=False)\n",
    "        self.bn = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, embedding=None, node_num=0):\n",
    "        out, (new_edge_index, att_weight) = self.gnn(x, edge_index, embedding, return_attention_weights=True)\n",
    "        self.att_weight_1 = att_weight\n",
    "        self.edge_index_1 = new_edge_index\n",
    "        out = self.bn(out)\n",
    "        return self.relu(out)\n",
    "\n",
    "class OutLayer(nn.Module):\n",
    "    def __init__(self, in_num, node_num, layer_num, inter_num = 512):\n",
    "        super(OutLayer, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(layer_num):\n",
    "            # last layer, output shape:1\n",
    "            if i==layer_num-1:\n",
    "                if layer_num==1:\n",
    "                    layers.append(nn.Linear(in_num , 1))\n",
    "                else:\n",
    "                    layers.append(nn.Linear(inter_num , 1))\n",
    "            else:\n",
    "                layer_in_num = in_num if i == 0 else inter_num\n",
    "                layers.append(nn.Linear( layer_in_num, inter_num ))\n",
    "                layers.append(nn.BatchNorm1d(inter_num))\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for mod in self.mlp:\n",
    "            if isinstance(mod, nn.BatchNorm1d):\n",
    "                out = out.permute(0,2,1)\n",
    "                out = mod(out)\n",
    "                out = out.permute(0,2,1)\n",
    "            else:\n",
    "                out = mod(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDN(nn.Module):\n",
    "    def __init__(self, edge_index_sets, node_num, dim=64, out_layer_inter_dim=256, input_dim=10, out_layer_num=1, topk=20):\n",
    "        super(GDN, self).__init__()\n",
    "        self.edge_index_sets = edge_index_sets\n",
    "        edge_index = edge_index_sets[0]\n",
    "        embed_dim = dim\n",
    "        self.embedding = nn.Embedding(node_num, embed_dim)\n",
    "        self.bn_outlayer_in = nn.BatchNorm1d(embed_dim)\n",
    "        edge_set_num = len(edge_index_sets)\n",
    "        \n",
    "        layers = []\n",
    "        for _ in range(edge_set_num):\n",
    "            gnn_layer = GNNLayer(input_dim, dim, inter_dim=dim+embed_dim, heads=1)\n",
    "            layers.append(gnn_layer)\n",
    "        \n",
    "        \n",
    "        self.gnn_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.node_embedding = None\n",
    "        self.topk = topk\n",
    "        self.learned_graph = None\n",
    "        self.out_layer = OutLayer(dim*edge_set_num, node_num, out_layer_num, inter_num = out_layer_inter_dim)\n",
    "        self.cache_edge_index_sets = [None] * edge_set_num\n",
    "        self.cache_embed_index = None\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, data, org_edge_index):\n",
    "        x = data.clone().detach()\n",
    "        edge_index_sets = self.edge_index_sets\n",
    "        device = data.device\n",
    "        \n",
    "        batch_num, node_num, all_feature = x.shape\n",
    "        x = x.view(-1, all_feature).contiguous()\n",
    "        gcn_outs = []\n",
    "        \n",
    "        for i, edge_index in enumerate(edge_index_sets):\n",
    "            edge_num = edge_index.shape[1]\n",
    "            cache_edge_index = self.cache_edge_index_sets[i]\n",
    "\n",
    "            if cache_edge_index is None or cache_edge_index.shape[1] != edge_num*batch_num:\n",
    "                self.cache_edge_index_sets[i] = get_batch_edge_index(edge_index, batch_num, node_num).to(device)\n",
    "            \n",
    "            batch_edge_index = self.cache_edge_index_sets[i]\n",
    "            all_embeddings = self.embedding(torch.arange(node_num).to(device))\n",
    "            #print(\"All embeddings: {}\".format(all_embeddings.shape))\n",
    "            weights_arr = all_embeddings.detach().clone()\n",
    "            all_embeddings = all_embeddings.repeat(batch_num, 1)\n",
    "            #print(\"All embeddings: {}\".format(all_embeddings.shape))\n",
    "            \n",
    "            weights = weights_arr.view(node_num, -1)\n",
    "            \n",
    "            #print(\"weights: {}\".format(weights.shape))\n",
    "            cos_ji_mat = torch.matmul(weights, weights.T)\n",
    "            normed_mat = torch.matmul(weights.norm(dim=-1).view(-1,1), weights.norm(dim=-1).view(1,-1))\n",
    "            cos_ji_mat = cos_ji_mat / normed_mat\n",
    "            dim = weights.shape[-1]\n",
    "            \n",
    "            topk_num = self.topk\n",
    "            topk_indices_ji = torch.topk(cos_ji_mat, topk_num, dim=-1)[1]\n",
    "            \n",
    "            #print(\"Topk_j_i shape {}\".format(topk_indices_ji.shape))\n",
    "            self.learned_graph = topk_indices_ji\n",
    "            \n",
    "            gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n",
    "            gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
    "            gated_edge_index = torch.cat((gated_j, gated_i), dim=0)\n",
    "            batch_gated_edge_index = get_batch_edge_index(gated_edge_index, batch_num, node_num).to(device)\n",
    "            #print(\"Shape of Batch Gated Edge Index: {}\".format(batch_gated_edge_index.shape))\n",
    "            gcn_out = self.gnn_layers[i](x, batch_gated_edge_index, node_num=node_num*batch_num, embedding=all_embeddings)\n",
    "            #print(\"Shape of gcn_out: {}\".format(gcn_out.shape))\n",
    "            gcn_outs.append(gcn_out)\n",
    "\n",
    "        x = torch.cat(gcn_outs, dim=1)\n",
    "        x = x.view(batch_num, node_num, -1)\n",
    "        \n",
    "        #print(\"X.shape is {}\".format(x.shape))\n",
    "        indexes = torch.arange(0,node_num).to(device)\n",
    "        #print(\"indexes.shape is {}\".format(indexes.shape))\n",
    "        out = torch.mul(x, self.embedding(indexes))\n",
    "        #print(\"out.shape is {}\".format(out.shape))\n",
    "        current_embedding = out.clone().detach()\n",
    "        out = out.permute(0,2,1)\n",
    "        out = F.relu(self.bn_outlayer_in(out))\n",
    "        out = out.permute(0,2,1)\n",
    "\n",
    "        out = self.dp(out)\n",
    "        #print(\"out_dropout.shape is {}\".format(out.shape))\n",
    "        out = self.out_layer(out)\n",
    "        #print(\"out_layer.shape is {}\".format(out.shape))\n",
    "        out = out.view(-1, node_num)\n",
    "        #print(\"out.shape is {}\".format(out.shape))\n",
    "        return out, current_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sDrVknvH0B7y"
   },
   "outputs": [],
   "source": [
    "def loss_func(y_pred, y_true):\n",
    "    loss = F.mse_loss(y_pred, y_true, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "def train_gdn(model, params,  train_dataloader, val_dataloader, feature_map, test_dataloader, test_dataset, train_dataset):\n",
    "    seed = params.seed\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay= params.decay)\n",
    "    train_loss_list = []\n",
    "    device = params.device\n",
    "    acu_loss = 0\n",
    "\n",
    "    i = 0\n",
    "    epoch = params.epoch\n",
    "    \n",
    "    early_stop_win = 15\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    dataloader = train_dataloader\n",
    "\n",
    "    for i_epoch in range(epoch):\n",
    "        acu_loss = 0\n",
    "        model.train()\n",
    "        for x, labels, attack_labels, edge_index in dataloader:\n",
    "            x, labels, edge_index = [item.float().to(device) for item in [x, labels, edge_index]]\n",
    "            #print(\"Shape of X is {}; Edge Index is {}; Labels is {}\".format(x.shape, edge_index.shape, labels.shape))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out, current_embeddings = model(x, edge_index)\n",
    "            out = out.float().to(device)\n",
    "            #print(\"Embedding Shape: {}\".format(current_embeddings.shape))\n",
    "            #print(\"Out shape {}\".format(out.shape))\n",
    "            loss = loss_func(out, labels)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(loss.item())\n",
    "            acu_loss += loss.item()\n",
    "            i += 1\n",
    "\n",
    "        print('epoch ({} / {}) (Loss:{:.8f}, ACU_loss:{:.8f})'.format(i_epoch, epoch, acu_loss/len(dataloader), acu_loss), flush=True)\n",
    "\n",
    "    return train_loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWvXrC560r8W"
   },
   "source": [
    "### Main Function\n",
    "<a id = \"driver\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters(object):\n",
    "    \"\"\"Class to store the parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "def get_loaders(train_dataset, seed, batch, val_ratio=0.1):\n",
    "    dataset_len = int(len(train_dataset))\n",
    "    train_use_len = int(dataset_len * (1 - val_ratio))\n",
    "    val_use_len = int(dataset_len * val_ratio)\n",
    "    val_start_index = random.randrange(train_use_len)\n",
    "    indices = torch.arange(dataset_len)\n",
    "\n",
    "    train_sub_indices = torch.cat([indices[:val_start_index], indices[val_start_index+val_use_len:]])\n",
    "    train_subset = Subset(train_dataset, train_sub_indices)\n",
    "\n",
    "    val_sub_indices = indices[val_start_index:val_start_index+val_use_len]\n",
    "    val_subset = Subset(train_dataset, val_sub_indices)\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_subset, batch_size=batch, shuffle=True)\n",
    "\n",
    "    val_dataloader = DataLoader(val_subset, batch_size=batch, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_13664\\3405506625.py:68: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2318.)\n",
      "  gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (0 / 50) (Loss:0.01793321, ACU_loss:34.21656924)\n",
      "epoch (1 / 50) (Loss:0.00998837, ACU_loss:19.05781708)\n",
      "epoch (2 / 50) (Loss:0.00981829, ACU_loss:18.73329959)\n",
      "epoch (3 / 50) (Loss:0.00979627, ACU_loss:18.69128987)\n",
      "epoch (4 / 50) (Loss:0.00948569, ACU_loss:18.09869299)\n",
      "epoch (5 / 50) (Loss:0.00947121, ACU_loss:18.07107746)\n",
      "epoch (6 / 50) (Loss:0.00884038, ACU_loss:16.86744813)\n",
      "epoch (7 / 50) (Loss:0.00879139, ACU_loss:16.77396823)\n",
      "epoch (8 / 50) (Loss:0.00860011, ACU_loss:16.40900251)\n",
      "epoch (9 / 50) (Loss:0.00897250, ACU_loss:17.11953620)\n",
      "epoch (10 / 50) (Loss:0.00827704, ACU_loss:15.79258474)\n",
      "epoch (11 / 50) (Loss:0.00819238, ACU_loss:15.63105811)\n",
      "epoch (12 / 50) (Loss:0.00833234, ACU_loss:15.89809779)\n",
      "epoch (13 / 50) (Loss:0.00828825, ACU_loss:15.81397978)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 62>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m edge_index_sets\u001b[38;5;241m.\u001b[39mappend(fc_edge_index)\n\u001b[0;32m     58\u001b[0m model \u001b[38;5;241m=\u001b[39m GDN(edge_index_sets, \u001b[38;5;28mlen\u001b[39m(feature_map), dim\u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mdim, input_dim\u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mslide_win,\n\u001b[0;32m     59\u001b[0m         out_layer_num\u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mout_layer_num, out_layer_inter_dim\u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mout_layer_inter_dim, topk\u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mtopk)\u001b[38;5;241m.\u001b[39mto(params\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 62\u001b[0m train_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gdn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mtrain_gdn\u001b[1;34m(model, params, train_dataloader, val_dataloader, feature_map, test_dataloader, test_dataset, train_dataset)\u001b[0m\n\u001b[0;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(out, labels)            \n\u001b[0;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 35\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m train_loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     37\u001b[0m acu_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\optim\\_functional.py:110\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    105\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    109\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m--> 110\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = Parameters()\n",
    "params.train_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/train.csv'\n",
    "params.test_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/test.csv'\n",
    "params.feature_list_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/list.txt'\n",
    "params.batch = 32\n",
    "params.epoch = 50\n",
    "params.slide_win = 5\n",
    "\n",
    "#embedding_dimension\n",
    "params.dim = 64\n",
    "\n",
    "params.slide_stride = 1\n",
    "params.device = 'cuda'\n",
    "params.seed = 5 #random_seed\n",
    "params.out_layer_num = 1\n",
    "params.out_layer_inter_dim = 128\n",
    "params.decay = 0\n",
    "params.val_ratio = 0.2\n",
    "params.topk = 5\n",
    "    \n",
    "train_orig = pd.read_csv(params.train_path, sep=',', index_col=0)\n",
    "test_orig = pd.read_csv(params.test_path, sep=',', index_col=0)\n",
    "feature_list_path = params.feature_list_path\n",
    "\n",
    "train, test = train_orig, test_orig\n",
    "\n",
    "if 'attack' in train.columns:\n",
    "    train = train.drop(columns=['attack'])\n",
    "\n",
    "#feature_map: List of sensor names. \n",
    "feature_map = get_feature_map(feature_list_path)\n",
    "\n",
    "#fc_struc: Discrionary for Dense graphs\n",
    "fc_struc = get_fc_graph_struc(feature_list_path)\n",
    "\n",
    "# fc_edge_index: edge_index for dense graph using fc_struc\n",
    "fc_edge_index = build_loc_net(fc_struc, list(train.columns), feature_map=feature_map)\n",
    "fc_edge_index = torch.tensor(fc_edge_index, dtype = torch.long)\n",
    "\n",
    "#train labels are set to 0\n",
    "train_dataset_indata = construct_data(train, feature_map, labels=0)\n",
    "test_dataset_indata = construct_data(test, feature_map, labels=test.attack.tolist())\n",
    "\n",
    "#generating features, labels in windows \n",
    "train_dataset = TimeDataset(train_dataset_indata, fc_edge_index, mode='train', stride = params.slide_stride, window_len = params.slide_win)\n",
    "test_dataset = TimeDataset(test_dataset_indata, fc_edge_index, mode='test', stride = params.slide_stride, window_len = params.slide_win)\n",
    "\n",
    "\n",
    "#data loaders\n",
    "train_dataloader, val_dataloader = get_loaders(train_dataset, params.seed, params.batch, params.val_ratio)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= params.batch, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "edge_index_sets = []\n",
    "edge_index_sets.append(fc_edge_index)\n",
    "\n",
    "model = GDN(edge_index_sets, len(feature_map), dim= params.dim, input_dim= params.slide_win,\n",
    "        out_layer_num= params.out_layer_num, out_layer_inter_dim= params.out_layer_inter_dim, topk= params.topk).to(params.device)\n",
    "\n",
    "\n",
    "train_log = train_gdn(model, params, train_dataloader, val_dataloader, feature_map, test_dataloader,test_dataset, train_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GDN_with_doc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
