{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dHdJIYKUmOxG"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pytz import utc, timezone\n",
    "\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import rankdata, iqr, trim_mean\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter, Linear, Sequential, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.nn import GCNConv, GATConv, EdgeConv\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyCa62G9fbXx"
   },
   "source": [
    "## **Contents**\n",
    "\n",
    "0.   Installations\n",
    "1.   [Why Graph Neural Networks](#whygnn)?\n",
    "2.   [Data Pre-Processing](#data_preprocessing)\n",
    "3.   [Utilities](#util)<br>\n",
    "        \n",
    "4.   [TimeDataset (Preparing to Forecast)](#TimeDataset)\n",
    "5.   [Graph Attention-Based Forecasting](#graph_layer)\n",
    "6.   [Graph Structure Learning + GDN](#gdn)\n",
    "7.   [Main](#driver)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2SmIBKbnYJq"
   },
   "source": [
    "### Why Graph Neural Networks?\n",
    "\n",
    "<a id = \"whygnn\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV4Z4V3tm-AF"
   },
   "source": [
    "\n",
    "1.   Given high-dimensional time series data (e.g., sensor data), how can we detect anomalous events ?, events such as system faults\n",
    "and attacks? More challenging, how can we do this in a way that captures complex inter-sensor relationships, and detects and explains anomalies which deviate from these relationships?\n",
    "2.   Capturing only the linear relationships is insufficient for complex, highly nonlinear relationships in many real-world settings. Data from these sensors can be related in complex, nonlinear ways. To learn representations for nonlinear high-dimensional time series and predict time series data, deep learning based time series methods have attracted interest in recent years.\n",
    "\n",
    "3.  In recent years, graph neural networks (GNNs) have emerged as successful approaches for modelling complex patterns in graph-structured data. In\n",
    "general, GNNs assume that the state of a node is influenced by the states of its neighbors. \n",
    "\n",
    "4. GNNs use the same model parameters to model the behavior of each node, and hence face limitations in representing very different behaviors of different sensors. Moreover, GNNs typically require the graph structure as an input, whereas the graph structure is initially unknown in many cases, and needs to be learned from data.\n",
    "\n",
    "5. GNNs are highly scalable as they use one model, and the same set of parameters to get the embeddings for all nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2LE1WGVCCYn"
   },
   "source": [
    "### Data Pre-Processing(Example using WADI data set, depending on the problem, it can be modified.)\n",
    "\n",
    "<a id = \"data_preprocessing\"> </a>\n",
    "\n",
    "\n",
    "1. We can modify the paths to training and testing data. \n",
    "2. Preprocessing involves missing data imputation with the mean values, normalizing the train/test data with MinMax Scaler. \n",
    "3. Downsample the data by 10 units(pick 1 timestep that represent 10 timesteps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EBGSYXiMxAzT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def norm(train, test):\n",
    "    normalizer = MinMaxScaler(feature_range=(0, 1)).fit(train) # scale training data to [0,1] range\n",
    "    train_ret = normalizer.transform(train)\n",
    "    test_ret = normalizer.transform(test)\n",
    "\n",
    "    return train_ret, test_ret\n",
    "\n",
    "\n",
    "def downsample(data, labels, down_len):\n",
    "    np_data = np.array(data)\n",
    "    np_labels = np.array(labels)\n",
    "\n",
    "    orig_len, col_num = np_data.shape\n",
    "    down_time_len = orig_len // down_len\n",
    "    np_data = np_data.transpose()\n",
    "    d_data = np_data[:, :down_time_len*down_len].reshape(col_num, -1, down_len)\n",
    "    d_data = np.median(d_data, axis=2).reshape(col_num, -1)\n",
    "\n",
    "    d_labels = np_labels[:down_time_len*down_len].reshape(-1, down_len)\n",
    "    # if exist anomalies, then this sample is abnormal\n",
    "    d_labels = np.round(np.max(d_labels, axis=1))\n",
    "\n",
    "    d_data = d_data.transpose()\n",
    "    return d_data.tolist(), d_labels.tolist()\n",
    "\n",
    "\n",
    "def pre_process(train_path, test_path, list_save_path, train_save_path, test_save_path):\n",
    "\n",
    "    train = pd.read_csv(train_path, index_col=0)\n",
    "    test = pd.read_csv(test_path, index_col=0)\n",
    "    train = train.iloc[:, 3:]\n",
    "    test = test.iloc[:, 3:]\n",
    "    train = train.fillna(train.mean())\n",
    "    test = test.fillna(test.mean())\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "\n",
    "    # trim column names\n",
    "    train = train.rename(columns=lambda x: x.strip())\n",
    "    test = test.rename(columns=lambda x: x.strip())\n",
    "\n",
    "    train_labels = np.zeros(len(train))\n",
    "    test_labels = test.attack\n",
    "\n",
    "\n",
    "    test = test.drop(columns=['attack'])\n",
    "    cols = train.columns\n",
    "    train.columns = cols\n",
    "    test.columns = cols\n",
    "\n",
    "    x_train, x_test = norm(train.values, test.values)\n",
    "\n",
    "    d_train_x, d_train_labels = downsample(x_train, train_labels, 10)\n",
    "    d_test_x, d_test_labels = downsample(x_test, test_labels, 10)\n",
    "\n",
    "    train_df = pd.DataFrame(d_train_x, columns = train.columns)\n",
    "    test_df = pd.DataFrame(d_test_x, columns = test.columns)\n",
    "\n",
    "\n",
    "    test_df['attack'] = d_test_labels\n",
    "    train_df['attack'] = d_train_labels\n",
    "\n",
    "    train_df = train_df.iloc[2160:]\n",
    "\n",
    "    train_df.to_csv(train_save_path)\n",
    "    test_df.to_csv(test_save_path)\n",
    "\n",
    "    f = open(list_save_path, 'w')\n",
    "    for col in train.columns:\n",
    "        f.write(col+'\\n')\n",
    "    f.close()\n",
    "\n",
    "train_path = \"C:/Users/harsh/OneDrive/Desktop/notebooks/WADI_14days.csv\"\n",
    "test_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/WADI_attackdata_labelled.csv'\n",
    "list_save_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/list.txt'\n",
    "\n",
    "train_save_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/train.csv'\n",
    "test_save_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/test.csv'\n",
    "\n",
    "\n",
    "pre_process(train_path, test_path, list_save_path, train_save_path, test_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnbNMHzFqQsc"
   },
   "source": [
    "### Utilities\n",
    "<a id = 'util'></a>\n",
    "\n",
    "Functions include helpers (assigning data to a device, **ex**: cuda or cpu), calculating scores(F1 etc) and statistics (median, IQR etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QUe5yQ4186-f"
   },
   "outputs": [],
   "source": [
    "def get_feature_map(feature_list_path):\n",
    "    feature_file = open(feature_list_path, 'r')\n",
    "    feature_list = []\n",
    "    for ft in feature_file:\n",
    "        feature_list.append(ft.strip())\n",
    "\n",
    "    return feature_list\n",
    "\n",
    "# graph is 'fully-connect'\n",
    "def get_fc_graph_struc(feature_list_path):\n",
    "    feature_file = open(feature_list_path, 'r')\n",
    "\n",
    "    struc_map = {}\n",
    "    feature_list = []\n",
    "    for ft in feature_file:\n",
    "        feature_list.append(ft.strip())\n",
    "\n",
    "    for ft in feature_list:\n",
    "        if ft not in struc_map:\n",
    "            struc_map[ft] = []\n",
    "\n",
    "        for other_ft in feature_list:\n",
    "            if other_ft is not ft:\n",
    "                struc_map[ft].append(other_ft)\n",
    "    \n",
    "    return struc_map\n",
    "\n",
    "\n",
    "def construct_data(data, feature_map, labels=0):\n",
    "    res = []\n",
    "\n",
    "    for feature in feature_map:\n",
    "        if feature in data.columns:\n",
    "            res.append(data.loc[:, feature].values.tolist())\n",
    "        else:\n",
    "            print(feature, 'not exist in data')\n",
    "    # append labels as last\n",
    "    sample_n = len(res[0])\n",
    "\n",
    "    if type(labels) == int:\n",
    "        res.append([labels]*sample_n)\n",
    "    elif len(labels) == sample_n:\n",
    "        res.append(labels)\n",
    "\n",
    "    return res\n",
    "\n",
    "def build_loc_net(struc, all_features, feature_map=[]):\n",
    "\n",
    "    index_feature_map = feature_map\n",
    "    edge_indexes = [[],[]]\n",
    "    for node_name, node_list in struc.items():\n",
    "        if node_name not in all_features:\n",
    "            continue\n",
    "\n",
    "        if node_name not in index_feature_map:\n",
    "            index_feature_map.append(node_name)\n",
    "        \n",
    "        p_index = index_feature_map.index(node_name)\n",
    "        for child in node_list:\n",
    "            if child not in all_features:\n",
    "                continue\n",
    "\n",
    "            if child not in index_feature_map:\n",
    "                print(f'error: {child} not in index_feature_map')\n",
    "                #index_feature_map.append(child)\n",
    "\n",
    "            c_index = index_feature_map.index(child)\n",
    "            edge_indexes[0].append(c_index)\n",
    "            edge_indexes[1].append(p_index)\n",
    "\n",
    "    return edge_indexes\n",
    "\n",
    "def get_batch_edge_index(org_edge_index, batch_num, node_num):\n",
    "    # org_edge_index:(2, edge_num)\n",
    "    edge_index = org_edge_index.clone().detach()\n",
    "    edge_num = org_edge_index.shape[1]\n",
    "    batch_edge_index = edge_index.repeat(1,batch_num).contiguous()\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        batch_edge_index[:, i*edge_num:(i+1)*edge_num] += i*node_num\n",
    "\n",
    "    return batch_edge_index.long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pQygPsadsMh"
   },
   "source": [
    "### **TimeDataset (Preparing to Forecast)**\n",
    "\n",
    "<a id='TimeDataset'></a>\n",
    "\n",
    "1.   Thus, at time t, define the model input $x(t) \\in \\mathbb{R}^{N \\times W}$\n",
    "based on a sliding window of size w over the historical time\n",
    "series data (whether training or testing data).\n",
    "\n",
    "\n",
    ">    <center>$x_{t} = [s^{(t - w)}, s^{(t - w + 1)} .... s^{(t - 1)}]$</center>\n",
    "\n",
    "<center> The target output that the model needs to predict is the sensor data at the current time tick, i.e. $s^{(t)}$. </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GET51afCykEc"
   },
   "outputs": [],
   "source": [
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, raw_data, edge_index, mode='train', config = None):\n",
    "        self.raw_data = raw_data\n",
    "        self.config = config\n",
    "        self.edge_index = edge_index\n",
    "        self.mode = mode\n",
    "\n",
    "        x_data = raw_data[:-1]\n",
    "        labels = raw_data[-1]\n",
    "        data = x_data\n",
    "        # to tensor\n",
    "        data = torch.tensor(data).double()\n",
    "        labels = torch.tensor(labels).double()\n",
    "\n",
    "        self.x, self.y, self.labels = self.process(data, labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "    def process(self, data, labels):\n",
    "        x_arr, y_arr = [], []\n",
    "        labels_arr = []\n",
    "\n",
    "        slide_win, slide_stride = [self.config[k] for k in ['slide_win', 'slide_stride']]\n",
    "        is_train = self.mode == 'train'\n",
    "        node_num, total_time_len = data.shape\n",
    "        rang = range(slide_win, total_time_len, slide_stride) if is_train else range(slide_win, total_time_len)\n",
    "        \n",
    "        for i in rang:\n",
    "            ft = data[:, i-slide_win:i]\n",
    "            tar = data[:, i]\n",
    "            x_arr.append(ft)\n",
    "            y_arr.append(tar)\n",
    "            labels_arr.append(labels[i])\n",
    "\n",
    "        x = torch.stack(x_arr).contiguous()\n",
    "        y = torch.stack(y_arr).contiguous()\n",
    "\n",
    "        labels = torch.Tensor(labels_arr).contiguous()\n",
    "        \n",
    "        return x, y, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.x[idx].double()\n",
    "        y = self.y[idx].double()\n",
    "        edge_index = self.edge_index.long()\n",
    "        label = self.labels[idx].double()\n",
    "\n",
    "        return feature, y, label, edge_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvho8te46CHv"
   },
   "source": [
    "### **Graph Attention-Based Forecasting**\n",
    "\n",
    "\n",
    "<a id='graph_layer'></a>\n",
    "\n",
    "\n",
    "1.   To capture the relationships between\n",
    "sensors, a graph attention-based feature extractor is introduced to fuse a node’s information with its neighbors based on\n",
    "the learned graph structure. Feature extractor incorporates the sensor\n",
    "embedding vectors $v_i$\n",
    ", which characterize the different behaviors of different types of sensors. To do this, compute\n",
    "node i’s aggregated representation $z_i$ as follows:\n",
    "\n",
    "<center>$z^{(t)}_{i} = ReLU(\\alpha_{i, i}\\textbf{W}x^{(t)}_i + \\sum_{j \\in N(i)} \\alpha_{i, j}\\textbf{W}x^{(t)}_j)$</center>\n",
    "\n",
    "\n",
    "<center>where $x^{(t)}_i \\in \\mathbb{R}^{w}$ is node i's input feature\n",
    "$N(i) = {j | A_{ji} > 0}$ is the set of neighbors of node i obtained from\n",
    "the learned adjacency matrix A, $W \\in \\mathbb{R}^{d \\times w}$ is a trainable\n",
    "weight matrix which applies a shared linear transformation to every node, and the attention coefficients $\\alpha_{i, j}$ are computed as:</center>\n",
    "\n",
    "<center>$g^{(t)}_{i} = v_{i} \\oplus \\textbf{W}x^{(t)}_{i}$</center>\n",
    "\n",
    "<center>$\\pi(i, j) = LeakyReLU(a^{T} (g^{(t)}_{i} \\oplus g^{(t)}_{j}))$</center>\n",
    "<center>$\\alpha(i, j) = SoftMax(\\pi(i, j))$</center>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Graph Structure Learning + GDN\n",
    "<a id = \"gdn\"></a>\n",
    "\n",
    "1.   A major goal of this framework is to learn the relationships\n",
    "between sensors in the form of a graph structure. To do this,\n",
    "a directed graph is used, whose nodes represent sensors, and whose edges represent dependency relationships\n",
    "between them.\n",
    "\n",
    "2. An edge from one sensor to another indicates\n",
    "that the first sensor is used for modelling the behavior of the\n",
    "second sensor. **A directed graph is used because the dependency patterns between sensors need not be symmetric.**\n",
    "\n",
    "3. A flexible framework is applied either to:<br>\n",
    "    3.1 The usual case where we have no prior information about the graph structure.<br>\n",
    "    \n",
    "    3.2 The case where we have some prior information about which edges are plausible (e.g. the sensor system may be divided into parts, where sensors in different parts have minimal interaction).<br>\n",
    "\n",
    "\n",
    "4. This prior information can be flexibly represented as a set\n",
    "of candidate relations $C_i$ for each sensor i, i.e. the sensors\n",
    "it could be dependent on:\n",
    "\n",
    "<center>{$C_i \\subset \\{1,3, 8, ... N\\}$ \\ ${i}$, no self loop.}</center>\n",
    "\n",
    "\n",
    "5. In the case without prior information, the candidate relations\n",
    "of sensor i is simply all sensors, other than itself.\n",
    "\n",
    "6.  The output of our algorithm is a set of $T_{test}$  binary labels\n",
    "indicating whether each test time tick is  = 1 an anomaly or not,\n",
    "i.e. $a(t) \\in \\{0, 1\\}$, where $a(t)$ indicates that time(t) is\n",
    "anomalous.\n",
    "\n",
    "7.  To select the dependencies of sensor i among these candidates, compute the similarity between node i’s embedding vector, and the embeddings of its candidates ${j \\in C_{i}}$\n",
    "\n",
    "That is, first compute $e_{ji}$, the normalized dot product between the embedding vectors of sensor i, and the candidate\n",
    "relation $j \\in C_{i}$\n",
    ". Then select the top k such normalized\n",
    "dot products: here **TopK** denotes the indices of top-k values among its input (i.e. the normalized dot products). **The value of k can be chosen by the user according to the desired sparsity level**. Next, a graph attention-based\n",
    "model is defined which makes use of this learned adjacency matrix A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KnV35iIlwVPD"
   },
   "outputs": [],
   "source": [
    "class GraphLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True, negative_slope=0.2, dropout=0, bias=True, inter_dim=-1,**kwargs):\n",
    "        super(GraphLayer, self).__init__(aggr='add', **kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.node_dim = 0\n",
    "        self.__alpha__ = None\n",
    "        self.lin = Linear(in_channels, heads * out_channels, bias=False)\n",
    "\n",
    "        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        glorot(self.att_i)\n",
    "        glorot(self.att_j)\n",
    "        zeros(self.att_em_i)\n",
    "        zeros(self.att_em_j)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, embedding, return_attention_weights=False):\n",
    "        if torch.is_tensor(x):\n",
    "            x = self.lin(x)\n",
    "            x = (x, x)\n",
    "        else:\n",
    "            x = (self.lin(x[0]), self.lin(x[1]))\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x[1].size(self.node_dim))\n",
    "        out = self.propagate(edge_index, x=x, embedding=embedding, edges=edge_index, return_attention_weights=return_attention_weights)\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if return_attention_weights:\n",
    "            alpha, self.__alpha__ = self.__alpha__, None\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index_i, size_i, embedding, edges, return_attention_weights):\n",
    "        x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding_i, embedding_j = embedding[edge_index_i], embedding[edges[0]]\n",
    "            embedding_i = embedding_i.unsqueeze(1).repeat(1,self.heads,1)\n",
    "            embedding_j = embedding_j.unsqueeze(1).repeat(1,self.heads,1)\n",
    "\n",
    "            key_i = torch.cat((x_i, embedding_i), dim=-1)\n",
    "            key_j = torch.cat((x_j, embedding_j), dim=-1)\n",
    "\n",
    "        cat_att_i = torch.cat((self.att_i, self.att_em_i), dim=-1)\n",
    "        cat_att_j = torch.cat((self.att_j, self.att_em_j), dim=-1)\n",
    "        alpha = (key_i * cat_att_i).sum(-1) + (key_j * cat_att_j).sum(-1)\n",
    "        alpha = alpha.view(-1, self.heads, 1)\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, edge_index_i, num_nodes = size_i)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            self.__alpha__ = alpha\n",
    "\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return x_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.heads)\n",
    "\n",
    "    \n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, inter_dim=0, heads=1, node_num=100):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.gnn = GraphLayer(in_channel, out_channel, inter_dim=inter_dim, heads=heads, concat=False)\n",
    "        self.bn = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, embedding=None, node_num=0):\n",
    "        out, (new_edge_index, att_weight) = self.gnn(x, edge_index, embedding, return_attention_weights=True)\n",
    "        self.att_weight_1 = att_weight\n",
    "        self.edge_index_1 = new_edge_index\n",
    "        out = self.bn(out)\n",
    "        return self.relu(out)\n",
    "\n",
    "class OutLayer(nn.Module):\n",
    "    def __init__(self, in_num, node_num, layer_num, inter_num = 512):\n",
    "        super(OutLayer, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(layer_num):\n",
    "            # last layer, output shape:1\n",
    "            if i==layer_num-1:\n",
    "                if layer_num==1:\n",
    "                    layers.append(nn.Linear(in_num , 1))\n",
    "                else:\n",
    "                    layers.append(nn.Linear(inter_num , 1))\n",
    "            else:\n",
    "                layer_in_num = in_num if i == 0 else inter_num\n",
    "                layers.append(nn.Linear( layer_in_num, inter_num ))\n",
    "                layers.append(nn.BatchNorm1d(inter_num))\n",
    "                layers.append(nn.ReLU())\n",
    "\n",
    "        self.mlp = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for mod in self.mlp:\n",
    "            if isinstance(mod, nn.BatchNorm1d):\n",
    "                out = out.permute(0,2,1)\n",
    "                out = mod(out)\n",
    "                out = out.permute(0,2,1)\n",
    "            else:\n",
    "                out = mod(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GDN(nn.Module):\n",
    "    def __init__(self, edge_index_sets, node_num, dim=64, out_layer_inter_dim=256, input_dim=10, out_layer_num=1, topk=20):\n",
    "        super(GDN, self).__init__()\n",
    "        self.edge_index_sets = edge_index_sets\n",
    "        device = 'cuda'\n",
    "        edge_index = edge_index_sets[0]\n",
    "        embed_dim = dim\n",
    "        self.embedding = nn.Embedding(node_num, embed_dim)\n",
    "        self.bn_outlayer_in = nn.BatchNorm1d(embed_dim)\n",
    "        edge_set_num = len(edge_index_sets)\n",
    "        \n",
    "        current_gnn_layers = []\n",
    "        for _ in range(edge_set_num):\n",
    "            gnn_layer = GNNLayer(input_dim, dim, inter_dim=dim+embed_dim, heads=1)\n",
    "            current_gnn_layers.append(gnn_layer)\n",
    "        \n",
    "        \n",
    "        self.gnn_layers = nn.ModuleList(current_gnn_layers)\n",
    "\n",
    "        self.node_embedding = None\n",
    "        self.topk = topk\n",
    "        self.learned_graph = None\n",
    "        self.out_layer = OutLayer(dim*edge_set_num, node_num, out_layer_num, inter_num = out_layer_inter_dim)\n",
    "        self.cache_edge_index_sets = [None] * edge_set_num\n",
    "        self.cache_embed_index = None\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, data, org_edge_index):\n",
    "        x = data.clone().detach()\n",
    "        edge_index_sets = self.edge_index_sets\n",
    "        device = data.device\n",
    "        batch_num, node_num, all_feature = x.shape\n",
    "        x = x.view(-1, all_feature).contiguous()\n",
    "        gcn_outs = []\n",
    "        for i, edge_index in enumerate(edge_index_sets):\n",
    "            edge_num = edge_index.shape[1]\n",
    "            cache_edge_index = self.cache_edge_index_sets[i]\n",
    "\n",
    "            if cache_edge_index is None or cache_edge_index.shape[1] != edge_num*batch_num:\n",
    "                self.cache_edge_index_sets[i] = get_batch_edge_index(edge_index, batch_num, node_num).to(device)\n",
    "            \n",
    "            batch_edge_index = self.cache_edge_index_sets[i]\n",
    "            all_embeddings = self.embedding(torch.arange(node_num).to(device))\n",
    "            weights_arr = all_embeddings.detach().clone()\n",
    "            all_embeddings = all_embeddings.repeat(batch_num, 1)\n",
    "            weights = weights_arr.view(node_num, -1)\n",
    "            cos_ji_mat = torch.matmul(weights, weights.T)\n",
    "            normed_mat = torch.matmul(weights.norm(dim=-1).view(-1,1), weights.norm(dim=-1).view(1,-1))\n",
    "            cos_ji_mat = cos_ji_mat / normed_mat\n",
    "            dim = weights.shape[-1]\n",
    "            topk_num = self.topk\n",
    "            topk_indices_ji = torch.topk(cos_ji_mat, topk_num, dim=-1)[1]\n",
    "            self.learned_graph = topk_indices_ji\n",
    "            gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n",
    "            gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
    "            gated_edge_index = torch.cat((gated_j, gated_i), dim=0)\n",
    "            batch_gated_edge_index = get_batch_edge_index(gated_edge_index, batch_num, node_num).to(device)\n",
    "            gcn_out = self.gnn_layers[i](x, batch_gated_edge_index, node_num=node_num*batch_num, embedding=all_embeddings)\n",
    "            gcn_outs.append(gcn_out)\n",
    "\n",
    "        x = torch.cat(gcn_outs, dim=1)\n",
    "        x = x.view(batch_num, node_num, -1)\n",
    "        \n",
    "        indexes = torch.arange(0,node_num).to(device)\n",
    "        out = torch.mul(x, self.embedding(indexes))\n",
    "        \n",
    "        out = out.permute(0,2,1)\n",
    "        out = F.relu(self.bn_outlayer_in(out))\n",
    "        out = out.permute(0,2,1)\n",
    "\n",
    "        out = self.dp(out)\n",
    "        out = self.out_layer(out)\n",
    "        out = out.view(-1, node_num)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "sDrVknvH0B7y"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_func(y_pred, y_true):\n",
    "    loss = F.mse_loss(y_pred, y_true, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "def train(model, config={},  train_dataloader=None, val_dataloader=None, feature_map={}, test_dataloader=None, test_dataset=None, dataset_name='swat', train_dataset=None):\n",
    "    seed = config['seed']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=config['decay'])\n",
    "    train_loss_list = []\n",
    "    device = 'cuda'\n",
    "    acu_loss = 0\n",
    "\n",
    "    i = 0\n",
    "    epoch = config['epoch']\n",
    "    early_stop_win = 15\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    dataloader = train_dataloader\n",
    "\n",
    "    for i_epoch in range(epoch):\n",
    "        acu_loss = 0\n",
    "        model.train()\n",
    "        for x, labels, attack_labels, edge_index in dataloader:\n",
    "            _start = time.time()\n",
    "            x, labels, edge_index = [item.float().to(device) for item in [x, labels, edge_index]]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x, edge_index).float().to(device)\n",
    "            loss = loss_func(out, labels)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(loss.item())\n",
    "            acu_loss += loss.item()\n",
    "            i += 1\n",
    "\n",
    "        print('epoch ({} / {}) (Loss:{:.8f}, ACU_loss:{:.8f})'.format(i_epoch, epoch, acu_loss/len(dataloader), acu_loss), flush=True)\n",
    "\n",
    "    return train_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWvXrC560r8W"
   },
   "source": [
    "### Main Function\n",
    "<a id = \"driver\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CL1X0U1TiZB",
    "outputId": "f8220942-b718-4079-e22c-37b8dc1c0b9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_15984\\1505769549.py:198: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2318.)\n",
      "  gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (0 / 10) (Loss:0.01976739, ACU_loss:37.71618425)\n",
      "epoch (1 / 10) (Loss:0.00992056, ACU_loss:18.92842144)\n",
      "epoch (2 / 10) (Loss:0.00906771, ACU_loss:17.30118730)\n",
      "epoch (3 / 10) (Loss:0.00857509, ACU_loss:16.36126775)\n",
      "epoch (4 / 10) (Loss:0.00865306, ACU_loss:16.51003857)\n",
      "epoch (5 / 10) (Loss:0.00865328, ACU_loss:16.51045281)\n",
      "epoch (6 / 10) (Loss:0.00864160, ACU_loss:16.48817449)\n",
      "epoch (7 / 10) (Loss:0.00830218, ACU_loss:15.84056009)\n",
      "epoch (8 / 10) (Loss:0.00815437, ACU_loss:15.55853944)\n",
      "epoch (9 / 10) (Loss:0.00805597, ACU_loss:15.37078172)\n"
     ]
    }
   ],
   "source": [
    "class Main(object):\n",
    "    def __init__(self, train_config, env_config, debug=False):\n",
    "        self.train_config = train_config\n",
    "        self.env_config = env_config\n",
    "        self.datestr = None\n",
    "        train_orig = pd.read_csv('C:/Users/harsh/OneDrive/Desktop/notebooks/train.csv', sep=',', index_col=0)\n",
    "        test_orig = pd.read_csv('C:/Users/harsh/OneDrive/Desktop/notebooks/test.csv', sep=',', index_col=0)\n",
    "        feature_list_path = 'C:/Users/harsh/OneDrive/Desktop/notebooks/list.txt'\n",
    "       \n",
    "        train, test = train_orig, test_orig\n",
    "\n",
    "        if 'attack' in train.columns:\n",
    "            train = train.drop(columns=['attack'])\n",
    "\n",
    "        feature_map = get_feature_map(feature_list_path)\n",
    "        fc_struc = get_fc_graph_struc(feature_list_path)\n",
    "        self.device = 'cuda'\n",
    "\n",
    "        fc_edge_index = build_loc_net(fc_struc, list(train.columns), feature_map=feature_map)\n",
    "        fc_edge_index = torch.tensor(fc_edge_index, dtype = torch.long)\n",
    "\n",
    "        self.feature_map = feature_map\n",
    "\n",
    "        train_dataset_indata = construct_data(train, feature_map, labels=0)\n",
    "        test_dataset_indata = construct_data(test, feature_map, labels=test.attack.tolist())\n",
    "\n",
    "\n",
    "        cfg = {\n",
    "            'slide_win': train_config['slide_win'],\n",
    "            'slide_stride': train_config['slide_stride'],\n",
    "        }\n",
    "\n",
    "        train_dataset = TimeDataset(train_dataset_indata, fc_edge_index, mode='train', config=cfg)\n",
    "        test_dataset = TimeDataset(test_dataset_indata, fc_edge_index, mode='test', config=cfg)\n",
    "\n",
    "\n",
    "        train_dataloader, val_dataloader = self.get_loaders(train_dataset, train_config['seed'], train_config['batch'], val_ratio = train_config['val_ratio'])\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = DataLoader(test_dataset, batch_size=train_config['batch'],\n",
    "                            shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "        edge_index_sets = []\n",
    "        edge_index_sets.append(fc_edge_index)\n",
    "\n",
    "        self.model = GDN(edge_index_sets, len(feature_map), \n",
    "                dim=train_config['dim'], \n",
    "                input_dim=train_config['slide_win'],\n",
    "                out_layer_num=train_config['out_layer_num'],\n",
    "                out_layer_inter_dim=train_config['out_layer_inter_dim'],\n",
    "                topk=train_config['topk']\n",
    "            ).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.train_log = train(self.model, \n",
    "                config = train_config,\n",
    "                train_dataloader=self.train_dataloader,\n",
    "                val_dataloader=self.val_dataloader, \n",
    "                feature_map=self.feature_map,\n",
    "                test_dataloader=self.test_dataloader,\n",
    "                test_dataset=self.test_dataset,\n",
    "                train_dataset=self.train_dataset,\n",
    "            )\n",
    "        \n",
    "    def get_loaders(self, train_dataset, seed, batch, val_ratio=0.1):\n",
    "        dataset_len = int(len(train_dataset))\n",
    "        train_use_len = int(dataset_len * (1 - val_ratio))\n",
    "        val_use_len = int(dataset_len * val_ratio)\n",
    "        val_start_index = random.randrange(train_use_len)\n",
    "        indices = torch.arange(dataset_len)\n",
    "\n",
    "        train_sub_indices = torch.cat([indices[:val_start_index], indices[val_start_index+val_use_len:]])\n",
    "        train_subset = Subset(train_dataset, train_sub_indices)\n",
    "\n",
    "        val_sub_indices = indices[val_start_index:val_start_index+val_use_len]\n",
    "        val_subset = Subset(train_dataset, val_sub_indices)\n",
    "\n",
    "\n",
    "        train_dataloader = DataLoader(train_subset, batch_size=batch,\n",
    "                                shuffle=True)\n",
    "\n",
    "        val_dataloader = DataLoader(val_subset, batch_size=batch,\n",
    "                                shuffle=False)\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch = 32\n",
    "    epoch = 10\n",
    "    slide_win = 5\n",
    "    dim = 64\n",
    "    slide_stride = 1\n",
    "    device = 'cuda'\n",
    "    random_seed = 5\n",
    "    out_layer_num = 1\n",
    "    out_layer_inter_dim = 128\n",
    "    decay = 0\n",
    "    val_ratio = 0.2\n",
    "    topk = 5\n",
    "    report = 'best'\n",
    "    load_model_path = ''\n",
    "\n",
    "    train_config = {\n",
    "        'batch': batch,\n",
    "        'epoch': epoch,\n",
    "        'slide_win': slide_win,\n",
    "        'dim': dim,\n",
    "        'slide_stride': slide_stride,\n",
    "        'seed': random_seed,\n",
    "        'out_layer_num': out_layer_num,\n",
    "        'out_layer_inter_dim': out_layer_inter_dim,\n",
    "        'decay': decay,\n",
    "        'val_ratio': val_ratio,\n",
    "        'topk': topk,\n",
    "    }\n",
    "\n",
    "    env_config={\n",
    "        'report': report,\n",
    "        'device': device,\n",
    "    }\n",
    "    \n",
    "\n",
    "    main = Main(train_config, env_config, debug=False)\n",
    "    main.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GDN_with_doc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
