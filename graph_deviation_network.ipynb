{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dHdJIYKUmOxG"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from numpy import percentile\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pytz import utc, timezone\n",
    "\n",
    "from scipy.stats import iqr\n",
    "from scipy.stats import rankdata, iqr, trim_mean\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter, Linear, Sequential, BatchNorm1d, ReLU\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.nn import GCNConv, GATConv, EdgeConv\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyCa62G9fbXx"
   },
   "source": [
    "## **Contents**\n",
    "\n",
    "1.   [Why Graph Neural Networks](#whygnn)?\n",
    "2.   [Data Pre-Processing](#data_preprocessing)\n",
    "3.   [Utilities](#util)<br>\n",
    "        \n",
    "4.   [TimeDataset (Preparing to Forecast)](#TimeDataset)\n",
    "5.   [Graph Attention-Based Forecasting](#graph_layer)\n",
    "6.   [Graph Structure Learning + GDN](#gdn)\n",
    "7.   [Main](#driver)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2SmIBKbnYJq"
   },
   "source": [
    "### Why Graph Neural Networks?\n",
    "\n",
    "<a id = \"whygnn\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV4Z4V3tm-AF"
   },
   "source": [
    "\n",
    "1.   Given high-dimensional time series data (e.g., sensor data), how can we detect anomalous events ?, events such as system faults\n",
    "and attacks? More challenging, how can we do this in a way that captures complex inter-sensor relationships, and detects and explains anomalies which deviate from these relationships?\n",
    "2.   Capturing only the linear relationships is insufficient for complex, highly nonlinear relationships in many real-world settings. Data from these sensors can be related in complex, nonlinear ways. To learn representations for nonlinear high-dimensional time series and predict time series data, deep learning based time series methods have attracted interest in recent years.\n",
    "\n",
    "3.  In recent years, graph neural networks (GNNs) have emerged as successful approaches for modelling complex patterns in graph-structured data. In\n",
    "general, GNNs assume that the state of a node is influenced by the states of its neighbors. \n",
    "\n",
    "4. GNNs use the same model parameters to model the behavior of each node, and hence face limitations in representing very different behaviors of different sensors. Moreover, GNNs typically require the graph structure as an input, whereas the graph structure is initially unknown in many cases, and needs to be learned from data.\n",
    "\n",
    "5. GNNs are highly scalable as they use one model, and the same set of parameters to get the embeddings for all nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W2LE1WGVCCYn"
   },
   "source": [
    "### Data Pre-Processing(Example using WADI data set, depending on the problem, it can be modified.)\n",
    "\n",
    "<a id = \"data_preprocessing\"> </a>\n",
    "\n",
    "\n",
    "1. We can modify the paths to training and testing data. \n",
    "2. Preprocessing involves missing data imputation with the mean values, normalizing the train/test data with MinMax Scaler. \n",
    "3. Downsample the data by 10 units(pick 1 timestep that represent 10 timesteps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EBGSYXiMxAzT"
   },
   "outputs": [],
   "source": [
    "def norm(train, test):\n",
    "    normalizer = MinMaxScaler(feature_range=(0, 1)).fit(train) # scale training data to [0,1] range\n",
    "    train_ret = normalizer.transform(train)\n",
    "    test_ret = normalizer.transform(test)\n",
    "\n",
    "    return train_ret, test_ret\n",
    "\n",
    "\n",
    "def downsample(data, labels, down_len):\n",
    "    np_data = np.array(data)\n",
    "    np_labels = np.array(labels)\n",
    "\n",
    "    orig_len, col_num = np_data.shape\n",
    "    down_time_len = orig_len // down_len\n",
    "    np_data = np_data.transpose()\n",
    "    d_data = np_data[:, :down_time_len*down_len].reshape(col_num, -1, down_len)\n",
    "    d_data = np.median(d_data, axis=2).reshape(col_num, -1)\n",
    "\n",
    "    d_labels = np_labels[:down_time_len*down_len].reshape(-1, down_len)\n",
    "    # if exist anomalies, then this sample is abnormal\n",
    "    d_labels = np.round(np.max(d_labels, axis=1))\n",
    "\n",
    "    d_data = d_data.transpose()\n",
    "    return d_data.tolist(), d_labels.tolist()\n",
    "\n",
    "\n",
    "def pre_process(train_path, test_path, list_save_path, train_save_path, test_save_path):\n",
    "\n",
    "    train = pd.read_csv(train_path, index_col=0)\n",
    "    test = pd.read_csv(test_path, index_col=0)\n",
    "    train = train.iloc[:, 3:]\n",
    "    test = test.iloc[:, 3:]\n",
    "    train = train.fillna(train.mean())\n",
    "    test = test.fillna(test.mean())\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "\n",
    "    # trim column names\n",
    "    train = train.rename(columns=lambda x: x.strip())\n",
    "    test = test.rename(columns=lambda x: x.strip())\n",
    "\n",
    "    train_labels = np.zeros(len(train))\n",
    "    test_labels = test.attack\n",
    "\n",
    "\n",
    "    test = test.drop(columns=['attack'])\n",
    "    cols = train.columns\n",
    "    train.columns = cols\n",
    "    test.columns = cols\n",
    "\n",
    "    x_train, x_test = norm(train.values, test.values)\n",
    "\n",
    "    d_train_x, d_train_labels = downsample(x_train, train_labels, 10)\n",
    "    d_test_x, d_test_labels = downsample(x_test, test_labels, 10)\n",
    "\n",
    "    train_df = pd.DataFrame(d_train_x, columns = train.columns)\n",
    "    test_df = pd.DataFrame(d_test_x, columns = test.columns)\n",
    "\n",
    "\n",
    "    test_df['attack'] = d_test_labels\n",
    "    train_df['attack'] = d_train_labels\n",
    "\n",
    "    train_df = train_df.iloc[2160:]\n",
    "\n",
    "    train_df.to_csv(train_save_path)\n",
    "    test_df.to_csv(test_save_path)\n",
    "\n",
    "    f = open(list_save_path, 'w')\n",
    "    for col in train.columns:\n",
    "        f.write(col+'\\n')\n",
    "    f.close()\n",
    "\n",
    "train_path = \"D:/papers/graphs/GDN/GDN/data/wadi/WADI_14days.csv\"\n",
    "test_path = \"D:/papers/graphs/GDN/GDN/data/wadi/WADI_attackdata_labelled.csv\"\n",
    "list_save_path = \"D:/papers/graphs/GDN/GDN/data/wadi/list.txt\"\n",
    "\n",
    "train_save_path = 'D:/papers/graphs/GDN/GDN/data/wadi/train.csv'\n",
    "test_save_path = 'D:/papers/graphs/GDN/GDN/data/wadi/test.csv'\n",
    "\n",
    "\n",
    "pre_process(train_path, test_path, list_save_path, train_save_path, test_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnbNMHzFqQsc"
   },
   "source": [
    "### Utilities\n",
    "<a id = 'util'></a>\n",
    "\n",
    "Functions include helpers (assigning data to a device, **ex**: cuda or cpu), calculating scores(F1 etc) and statistics (median, IQR etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QUe5yQ4186-f"
   },
   "outputs": [],
   "source": [
    "# calculate F1 scores\n",
    "def eval_scores(scores, true_scores, th_steps, return_thresold=False):\n",
    "    padding_list = [0]*(len(true_scores) - len(scores))\n",
    "    if len(padding_list) > 0:\n",
    "        scores = padding_list + scores\n",
    "\n",
    "    scores_sorted = rankdata(scores, method='ordinal')\n",
    "    th_steps = th_steps\n",
    "    # th_steps = 500\n",
    "    th_vals = np.array(range(th_steps)) * 1.0 / th_steps\n",
    "    fmeas = [None] * th_steps\n",
    "    thresholds = [None] * th_steps\n",
    "    for i in range(th_steps):\n",
    "        cur_pred = scores_sorted > th_vals[i] * len(scores)\n",
    "\n",
    "        fmeas[i] = f1_score(true_scores, cur_pred)\n",
    "\n",
    "        score_index = scores_sorted.tolist().index(int(th_vals[i] * len(scores)+1))\n",
    "        thresholds[i] = scores[score_index]\n",
    "\n",
    "    if return_thresold:\n",
    "        return fmeas, thresholds\n",
    "    return fmeas\n",
    "\n",
    "def eval_mseloss(predicted, ground_truth):\n",
    "\n",
    "    ground_truth_list = np.array(ground_truth)\n",
    "    predicted_list = np.array(predicted)\n",
    "    loss = mean_squared_error(predicted_list, ground_truth_list)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def get_err_median_and_iqr(predicted, groundtruth):\n",
    "\n",
    "    np_arr = np.abs(np.subtract(np.array(predicted), np.array(groundtruth)))\n",
    "\n",
    "    err_median = np.median(np_arr)\n",
    "    err_iqr = iqr(np_arr)\n",
    "\n",
    "    return err_median, err_iqr\n",
    "\n",
    "def get_err_median_and_quantile(predicted, groundtruth, percentage):\n",
    "\n",
    "    np_arr = np.abs(np.subtract(np.array(predicted), np.array(groundtruth)))\n",
    "\n",
    "    err_median = np.median(np_arr)\n",
    "    # err_iqr = iqr(np_arr)\n",
    "    err_delta = percentile(np_arr, int(percentage*100)) - percentile(np_arr, int((1-percentage)*100))\n",
    "\n",
    "    return err_median, err_delta\n",
    "\n",
    "def get_err_mean_and_quantile(predicted, groundtruth, percentage):\n",
    "\n",
    "    np_arr = np.abs(np.subtract(np.array(predicted), np.array(groundtruth)))\n",
    "\n",
    "    err_median = trim_mean(np_arr, percentage)\n",
    "    # err_iqr = iqr(np_arr)\n",
    "    err_delta = percentile(np_arr, int(percentage*100)) - percentile(np_arr, int((1-percentage)*100))\n",
    "\n",
    "    return err_median, err_delta\n",
    "\n",
    "def get_err_mean_and_std(predicted, groundtruth):\n",
    "\n",
    "    np_arr = np.abs(np.subtract(np.array(predicted), np.array(groundtruth)))\n",
    "\n",
    "    err_mean = np.mean(np_arr)\n",
    "    err_std = np.std(np_arr)\n",
    "\n",
    "    return err_mean, err_std\n",
    "\n",
    "\n",
    "def get_f1_score(scores, gt, contamination):\n",
    "    padding_list = [0]*(len(gt) - len(scores))\n",
    "    threshold = percentile(scores, 100 * (1 - contamination))\n",
    "\n",
    "    if len(padding_list) > 0:\n",
    "        scores = padding_list + scores\n",
    "\n",
    "    pred_labels = (scores > threshold).astype('int').ravel()\n",
    "\n",
    "    return f1_score(gt, pred_labels)\n",
    "\n",
    "\n",
    "\n",
    "_device = 'cuda' \n",
    "\n",
    "def get_device():\n",
    "    return _device\n",
    "\n",
    "def set_device(dev):\n",
    "    global _device\n",
    "    _device = dev\n",
    "\n",
    "def init_work(worker_id, seed):\n",
    "    np.random.seed(seed + worker_id)\n",
    "\n",
    "def printsep():\n",
    "    print('='*40+'\\n')\n",
    "\n",
    "\n",
    "def get_feature_map(feature_list_path):\n",
    "    feature_file = open(feature_list_path, 'r')\n",
    "    feature_list = []\n",
    "    for ft in feature_file:\n",
    "        feature_list.append(ft.strip())\n",
    "\n",
    "    return feature_list\n",
    "\n",
    "# graph is 'fully-connect'\n",
    "def get_fc_graph_struc(feature_list_path):\n",
    "    feature_file = open(feature_list_path, 'r')\n",
    "\n",
    "    struc_map = {}\n",
    "    feature_list = []\n",
    "    for ft in feature_file:\n",
    "        feature_list.append(ft.strip())\n",
    "\n",
    "    for ft in feature_list:\n",
    "        if ft not in struc_map:\n",
    "            struc_map[ft] = []\n",
    "\n",
    "        for other_ft in feature_list:\n",
    "            if other_ft is not ft:\n",
    "                struc_map[ft].append(other_ft)\n",
    "    \n",
    "    return struc_map\n",
    "\n",
    "\n",
    "def construct_data(data, feature_map, labels=0):\n",
    "    res = []\n",
    "\n",
    "    for feature in feature_map:\n",
    "        if feature in data.columns:\n",
    "            res.append(data.loc[:, feature].values.tolist())\n",
    "        else:\n",
    "            print(feature, 'not exist in data')\n",
    "    # append labels as last\n",
    "    sample_n = len(res[0])\n",
    "\n",
    "    if type(labels) == int:\n",
    "        res.append([labels]*sample_n)\n",
    "    elif len(labels) == sample_n:\n",
    "        res.append(labels)\n",
    "\n",
    "    return res\n",
    "\n",
    "def build_loc_net(struc, all_features, feature_map=[]):\n",
    "\n",
    "    index_feature_map = feature_map\n",
    "    edge_indexes = [[],[]]\n",
    "    for node_name, node_list in struc.items():\n",
    "        if node_name not in all_features:\n",
    "            continue\n",
    "\n",
    "        if node_name not in index_feature_map:\n",
    "            index_feature_map.append(node_name)\n",
    "        \n",
    "        p_index = index_feature_map.index(node_name)\n",
    "        for child in node_list:\n",
    "            if child not in all_features:\n",
    "                continue\n",
    "\n",
    "            if child not in index_feature_map:\n",
    "                print(f'error: {child} not in index_feature_map')\n",
    "                #index_feature_map.append(child)\n",
    "\n",
    "            c_index = index_feature_map.index(child)\n",
    "            edge_indexes[0].append(c_index)\n",
    "            edge_indexes[1].append(p_index)\n",
    "\n",
    "    return edge_indexes\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSincePlus(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timestamp2str(sec, fmt, tz):\n",
    "    return datetime.fromtimestamp(sec).astimezone(tz).strftime(fmt)\n",
    "\n",
    "\n",
    "def get_full_err_scores(test_result, val_result):\n",
    "    np_test_result = np.array(test_result)\n",
    "    np_val_result = np.array(val_result)\n",
    "\n",
    "    all_scores =  None\n",
    "    all_normals = None\n",
    "    feature_num = np_test_result.shape[-1]\n",
    "\n",
    "    labels = np_test_result[2, :, 0].tolist()\n",
    "\n",
    "    for i in range(feature_num):\n",
    "        test_re_list = np_test_result[:2,:,i]\n",
    "        val_re_list = np_val_result[:2,:,i]\n",
    "\n",
    "        scores = get_err_scores(test_re_list, val_re_list)\n",
    "        normal_dist = get_err_scores(val_re_list, val_re_list)\n",
    "\n",
    "        if all_scores is None:\n",
    "            all_scores = scores\n",
    "            all_normals = normal_dist\n",
    "        else:\n",
    "            all_scores = np.vstack((all_scores, scores))\n",
    "            all_normals = np.vstack((all_normals, normal_dist))\n",
    "\n",
    "    return all_scores, all_normals\n",
    "\n",
    "\n",
    "def get_final_err_scores(test_result, val_result):\n",
    "    full_scores, all_normals = get_full_err_scores(test_result, val_result, return_normal_scores=True)\n",
    "\n",
    "    all_scores = np.max(full_scores, axis=0)\n",
    "\n",
    "    return all_scores\n",
    "\n",
    "\n",
    "\n",
    "def get_err_scores(test_res, val_res):\n",
    "    test_predict, test_gt = test_res\n",
    "    val_predict, val_gt = val_res\n",
    "\n",
    "    n_err_mid, n_err_iqr = get_err_median_and_iqr(test_predict, test_gt)\n",
    "\n",
    "    test_delta = np.abs(np.subtract(\n",
    "                        np.array(test_predict).astype(np.float64), \n",
    "                        np.array(test_gt).astype(np.float64)\n",
    "                    ))\n",
    "    epsilon=1e-2\n",
    "\n",
    "    err_scores = (test_delta - n_err_mid) / ( np.abs(n_err_iqr) +epsilon)\n",
    "\n",
    "    smoothed_err_scores = np.zeros(err_scores.shape)\n",
    "    before_num = 3\n",
    "    for i in range(before_num, len(err_scores)):\n",
    "        smoothed_err_scores[i] = np.mean(err_scores[i-before_num:i+1])\n",
    "\n",
    "    \n",
    "    return smoothed_err_scores\n",
    "\n",
    "\n",
    "\n",
    "def get_loss(predict, gt):\n",
    "    return eval_mseloss(predict, gt)\n",
    "\n",
    "def get_f1_scores(total_err_scores, gt_labels, topk=1):\n",
    "    print('total_err_scores', total_err_scores.shape)\n",
    "    # remove the highest and lowest score at each timestep\n",
    "    total_features = total_err_scores.shape[0]\n",
    "\n",
    "    # topk_indices = np.argpartition(total_err_scores, range(total_features-1-topk, total_features-1), axis=0)[-topk-1:-1]\n",
    "    topk_indices = np.argpartition(total_err_scores, range(total_features-topk-1, total_features), axis=0)[-topk:]\n",
    "    \n",
    "    topk_indices = np.transpose(topk_indices)\n",
    "\n",
    "    total_topk_err_scores = []\n",
    "    topk_err_score_map=[]\n",
    "    # topk_anomaly_sensors = []\n",
    "    for i, indexs in enumerate(topk_indices):\n",
    "        sum_score = sum(score for k, score in enumerate(sorted([total_err_scores[index, i] for j, index in enumerate(indexs)])) )\n",
    "        total_topk_err_scores.append(sum_score)\n",
    "\n",
    "    final_topk_fmeas = eval_scores(total_topk_err_scores, gt_labels, 400)\n",
    "\n",
    "    return final_topk_fmeas\n",
    "\n",
    "def get_val_performance_data(total_err_scores, normal_scores, gt_labels, topk=1):\n",
    "    total_features = total_err_scores.shape[0]\n",
    "\n",
    "    topk_indices = np.argpartition(total_err_scores, range(total_features-topk-1, total_features), axis=0)[-topk:]\n",
    "\n",
    "    total_topk_err_scores = []\n",
    "    topk_err_score_map=[]\n",
    "\n",
    "    total_topk_err_scores = np.sum(np.take_along_axis(total_err_scores, topk_indices, axis=0), axis=0)\n",
    "\n",
    "    thresold = np.max(normal_scores)\n",
    "\n",
    "    pred_labels = np.zeros(len(total_topk_err_scores))\n",
    "    pred_labels[total_topk_err_scores > thresold] = 1\n",
    "\n",
    "    for i in range(len(pred_labels)):\n",
    "        pred_labels[i] = int(pred_labels[i])\n",
    "        gt_labels[i] = int(gt_labels[i])\n",
    "\n",
    "    pre = precision_score(gt_labels, pred_labels)\n",
    "    rec = recall_score(gt_labels, pred_labels)\n",
    "\n",
    "    f1 = f1_score(gt_labels, pred_labels)\n",
    "\n",
    "\n",
    "    auc_score = roc_auc_score(gt_labels, total_topk_err_scores)\n",
    "\n",
    "    return f1, pre, rec, auc_score, thresold\n",
    "\n",
    "\n",
    "def get_best_performance_data(total_err_scores, gt_labels, topk=1):\n",
    "    total_features = total_err_scores.shape[0]\n",
    "    topk_indices = np.argpartition(total_err_scores, range(total_features-topk-1, total_features), axis=0)[-topk:]\n",
    "\n",
    "    total_topk_err_scores = []\n",
    "    topk_err_score_map=[]\n",
    "\n",
    "    total_topk_err_scores = np.sum(np.take_along_axis(total_err_scores, topk_indices, axis=0), axis=0)\n",
    "\n",
    "    final_topk_fmeas ,thresolds = eval_scores(total_topk_err_scores, gt_labels, 400, return_thresold=True)\n",
    "\n",
    "    th_i = final_topk_fmeas.index(max(final_topk_fmeas))\n",
    "    thresold = thresolds[th_i]\n",
    "\n",
    "    pred_labels = np.zeros(len(total_topk_err_scores))\n",
    "    pred_labels[total_topk_err_scores > thresold] = 1\n",
    "\n",
    "    for i in range(len(pred_labels)):\n",
    "        pred_labels[i] = int(pred_labels[i])\n",
    "        gt_labels[i] = int(gt_labels[i])\n",
    "\n",
    "    pre = precision_score(gt_labels, pred_labels)\n",
    "    rec = recall_score(gt_labels, pred_labels)\n",
    "\n",
    "    auc_score = roc_auc_score(gt_labels, total_topk_err_scores)\n",
    "\n",
    "    return max(final_topk_fmeas), pre, rec, auc_score, thresold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pQygPsadsMh"
   },
   "source": [
    "### **TimeDataset (Preparing to Forecast)**\n",
    "\n",
    "<a id='TimeDataset'></a>\n",
    "\n",
    "1.   Thus, at time t, define the model input $x(t) \\in \\mathbb{R}^{N \\times W}$\n",
    "based on a sliding window of size w over the historical time\n",
    "series data (whether training or testing data).\n",
    "\n",
    "\n",
    ">    <center>$x_{t} = [s^{(t - w)}, s^{(t - w + 1)} .... s^{(t - 1)}]$</center>\n",
    "\n",
    "<center> The target output that the model needs to predict is the sensor data at the current time tick, i.e. $s^{(t)}$. </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GET51afCykEc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TimeDataset(Dataset):\n",
    "    def __init__(self, raw_data, edge_index, mode='train', config = None):\n",
    "        self.raw_data = raw_data\n",
    "        self.config = config\n",
    "        self.edge_index = edge_index\n",
    "        self.mode = mode\n",
    "\n",
    "        x_data = raw_data[:-1]\n",
    "        labels = raw_data[-1]\n",
    "        data = x_data\n",
    "        # to tensor\n",
    "        data = torch.tensor(data).double()\n",
    "        labels = torch.tensor(labels).double()\n",
    "\n",
    "        self.x, self.y, self.labels = self.process(data, labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "\n",
    "    def process(self, data, labels):\n",
    "        x_arr, y_arr = [], []\n",
    "        labels_arr = []\n",
    "\n",
    "        slide_win, slide_stride = [self.config[k] for k in ['slide_win', 'slide_stride']]\n",
    "        is_train = self.mode == 'train'\n",
    "        node_num, total_time_len = data.shape\n",
    "        rang = range(slide_win, total_time_len, slide_stride) if is_train else range(slide_win, total_time_len)\n",
    "        \n",
    "        for i in rang:\n",
    "            ft = data[:, i-slide_win:i]\n",
    "            tar = data[:, i]\n",
    "            x_arr.append(ft)\n",
    "            y_arr.append(tar)\n",
    "            labels_arr.append(labels[i])\n",
    "\n",
    "        x = torch.stack(x_arr).contiguous()\n",
    "        y = torch.stack(y_arr).contiguous()\n",
    "\n",
    "        labels = torch.Tensor(labels_arr).contiguous()\n",
    "        \n",
    "        return x, y, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.x[idx].double()\n",
    "        y = self.y[idx].double()\n",
    "        edge_index = self.edge_index.long()\n",
    "        label = self.labels[idx].double()\n",
    "\n",
    "        return feature, y, label, edge_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvho8te46CHv"
   },
   "source": [
    "### **Graph Attention-Based Forecasting**\n",
    "\n",
    "\n",
    "<a id='graph_layer'></a>\n",
    "\n",
    "\n",
    "1.   To capture the relationships between\n",
    "sensors, a graph attention-based feature extractor is introduced to fuse a node’s information with its neighbors based on\n",
    "the learned graph structure. Feature extractor incorporates the sensor\n",
    "embedding vectors $v_i$\n",
    ", which characterize the different behaviors of different types of sensors. To do this, compute\n",
    "node i’s aggregated representation $z_i$ as follows:\n",
    "\n",
    "<center>$z^{(t)}_{i} = ReLU(\\alpha_{i, i}\\textbf{W}x^{(t)}_i + \\sum_{j \\in N(i)} \\alpha_{i, j}\\textbf{W}x^{(t)}_j)$</center>\n",
    "\n",
    "\n",
    "<center>where $x^{(t)}_i \\in \\mathbb{R}^{w}$ is node i's input feature\n",
    "$N(i) = {j | A_{ji} > 0}$ is the set of neighbors of node i obtained from\n",
    "the learned adjacency matrix A, $W \\in \\mathbb{R}^{d \\times w}$ is a trainable\n",
    "weight matrix which applies a shared linear transformation to every node, and the attention coefficients $\\alpha_{i, j}$ are computed as:</center>\n",
    "\n",
    "<center>$g^{(t)}_{i} = v_{i} \\oplus \\textbf{W}x^{(t)}_{i}$</center>\n",
    "\n",
    "<center>$\\pi(i, j) = LeakyReLU(a^{T} (g^{(t)}_{i} \\oplus g^{(t)}_{j}))$</center>\n",
    "<center>$\\alpha(i, j) = SoftMax(\\pi(i, j))$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KnV35iIlwVPD"
   },
   "outputs": [],
   "source": [
    "class GraphLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True, negative_slope=0.2, dropout=0, bias=True, inter_dim=-1,**kwargs):\n",
    "        super(GraphLayer, self).__init__(aggr='add', **kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.node_dim = 0\n",
    "        self.__alpha__ = None\n",
    "        self.lin = Linear(in_channels, heads * out_channels, bias=False)\n",
    "\n",
    "        self.att_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_i = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_em_j = Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin.weight)\n",
    "        glorot(self.att_i)\n",
    "        glorot(self.att_j)\n",
    "        zeros(self.att_em_i)\n",
    "        zeros(self.att_em_j)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, edge_index, embedding, return_attention_weights=False):\n",
    "        if torch.is_tensor(x):\n",
    "            x = self.lin(x)\n",
    "            x = (x, x)\n",
    "        else:\n",
    "            x = (self.lin(x[0]), self.lin(x[1]))\n",
    "\n",
    "        edge_index, _ = remove_self_loops(edge_index)\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x[1].size(self.node_dim))\n",
    "        out = self.propagate(edge_index, x=x, embedding=embedding, edges=edge_index, return_attention_weights=return_attention_weights)\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "\n",
    "        if return_attention_weights:\n",
    "            alpha, self.__alpha__ = self.__alpha__, None\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_index_i, size_i, embedding, edges, return_attention_weights):\n",
    "        x_i = x_i.view(-1, self.heads, self.out_channels)\n",
    "        x_j = x_j.view(-1, self.heads, self.out_channels)\n",
    "\n",
    "        if embedding is not None:\n",
    "            embedding_i, embedding_j = embedding[edge_index_i], embedding[edges[0]]\n",
    "            embedding_i = embedding_i.unsqueeze(1).repeat(1,self.heads,1)\n",
    "            embedding_j = embedding_j.unsqueeze(1).repeat(1,self.heads,1)\n",
    "\n",
    "            key_i = torch.cat((x_i, embedding_i), dim=-1)\n",
    "            key_j = torch.cat((x_j, embedding_j), dim=-1)\n",
    "\n",
    "        cat_att_i = torch.cat((self.att_i, self.att_em_i), dim=-1)\n",
    "        cat_att_j = torch.cat((self.att_j, self.att_em_j), dim=-1)\n",
    "        alpha = (key_i * cat_att_i).sum(-1) + (key_j * cat_att_j).sum(-1)\n",
    "        alpha = alpha.view(-1, self.heads, 1)\n",
    "        alpha = F.leaky_relu(alpha, self.negative_slope)\n",
    "        alpha = softmax(alpha, edge_index_i, num_nodes = size_i)\n",
    "\n",
    "        if return_attention_weights:\n",
    "            self.__alpha__ = alpha\n",
    "\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        \n",
    "        return x_j * alpha.view(-1, self.heads, 1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCrSsZF3uLNU"
   },
   "source": [
    "\n",
    "### Graph Structure Learning + GDN\n",
    "<a id = \"gdn\"></a>\n",
    "\n",
    "1.   A major goal of this framework is to learn the relationships\n",
    "between sensors in the form of a graph structure. To do this,\n",
    "a directed graph is used, whose nodes represent sensors, and whose edges represent dependency relationships\n",
    "between them.\n",
    "\n",
    "2. An edge from one sensor to another indicates\n",
    "that the first sensor is used for modelling the behavior of the\n",
    "second sensor. **A directed graph is used because the dependency patterns between sensors need not be symmetric.**\n",
    "\n",
    "3. A flexible framework is applied either to:<br>\n",
    "    3.1 The usual case where we have no prior information about the graph structure.<br>\n",
    "    \n",
    "    3.2 The case where we have some prior information about which edges are plausible (e.g. the sensor system may be divided into parts, where sensors in different parts have minimal interaction).<br>\n",
    "\n",
    "\n",
    "4. This prior information can be flexibly represented as a set\n",
    "of candidate relations $C_i$ for each sensor i, i.e. the sensors\n",
    "it could be dependent on:\n",
    "\n",
    "<center>{$C_i \\subset \\{1,3, 8, ... N\\}$ \\ ${i}$, no self loop.}</center>\n",
    "\n",
    "\n",
    "5. In the case without prior information, the candidate relations\n",
    "of sensor i is simply all sensors, other than itself.\n",
    "\n",
    "6.  The output of our algorithm is a set of $T_{test}$  binary labels\n",
    "indicating whether each test time tick is  = 1 an anomaly or not,\n",
    "i.e. $a(t) \\in \\{0, 1\\}$, where $a(t)$ indicates that time(t) is\n",
    "anomalous.\n",
    "\n",
    "7.  To select the dependencies of sensor i among these candidates, compute the similarity between node i’s embedding vector, and the embeddings of its candidates ${j \\in C_{i}}$\n",
    "\n",
    "That is, first compute $e_{ji}$, the normalized dot product between the embedding vectors of sensor i, and the candidate\n",
    "relation $j \\in C_{i}$\n",
    ". Then select the top k such normalized\n",
    "dot products: here **TopK** denotes the indices of top-k values among its input (i.e. the normalized dot products). **The value of k can be chosen by the user according to the desired sparsity level**. Next, a graph attention-based\n",
    "model is defined which makes use of this learned adjacency matrix A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "X3gkptYFwbHG"
   },
   "outputs": [],
   "source": [
    "def get_batch_edge_index(org_edge_index, batch_num, node_num):\n",
    "    # org_edge_index:(2, edge_num)\n",
    "    edge_index = org_edge_index.clone().detach()\n",
    "    edge_num = org_edge_index.shape[1]\n",
    "    batch_edge_index = edge_index.repeat(1,batch_num).contiguous()\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        batch_edge_index[:, i*edge_num:(i+1)*edge_num] += i*node_num\n",
    "\n",
    "    return batch_edge_index.long()\n",
    "\n",
    "\n",
    "class OutLayer(nn.Module):\n",
    "    def __init__(self, in_num, node_num, layer_num, inter_num = 512):\n",
    "        super(OutLayer, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "\n",
    "        for i in range(layer_num):\n",
    "            # last layer, output shape:1\n",
    "            if i == layer_num-1:\n",
    "                modules.append(nn.Linear( in_num if layer_num == 1 else inter_num, 1))\n",
    "            else:\n",
    "                layer_in_num = in_num if i == 0 else inter_num\n",
    "                modules.append(nn.Linear( layer_in_num, inter_num ))\n",
    "                modules.append(nn.BatchNorm1d(inter_num))\n",
    "                modules.append(nn.ReLU())\n",
    "\n",
    "        self.mlp = nn.ModuleList(modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for mod in self.mlp:\n",
    "            if isinstance(mod, nn.BatchNorm1d):\n",
    "                out = out.permute(0,2,1)\n",
    "                out = mod(out)\n",
    "                out = out.permute(0,2,1)\n",
    "            else:\n",
    "                out = mod(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, inter_dim=0, heads=1, node_num=100):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.gnn = GraphLayer(in_channel, out_channel, inter_dim=inter_dim, heads=heads, concat=False)\n",
    "        self.bn = nn.BatchNorm1d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leaky_relu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x, edge_index, embedding=None, node_num=0):\n",
    "        out, (new_edge_index, att_weight) = self.gnn(x, edge_index, embedding, return_attention_weights=True)\n",
    "        self.att_weight_1 = att_weight\n",
    "        self.edge_index_1 = new_edge_index\n",
    "        out = self.bn(out)\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class GDN(nn.Module):\n",
    "    def __init__(self, edge_index_sets, node_num, dim=64, out_layer_inter_dim=256, input_dim=10, out_layer_num=1, topk=20):\n",
    "        super(GDN, self).__init__()\n",
    "        self.edge_index_sets = edge_index_sets\n",
    "        device = get_device()\n",
    "        edge_index = edge_index_sets[0]\n",
    "        embed_dim = dim\n",
    "        self.embedding = nn.Embedding(node_num, embed_dim)\n",
    "        self.bn_outlayer_in = nn.BatchNorm1d(embed_dim)\n",
    "        edge_set_num = len(edge_index_sets)\n",
    "        self.gnn_layers = nn.ModuleList([\n",
    "            GNNLayer(input_dim, dim, inter_dim=dim+embed_dim, heads=1) for i in range(edge_set_num)\n",
    "        ])\n",
    "\n",
    "        self.node_embedding = None\n",
    "        self.topk = topk\n",
    "        self.learned_graph = None\n",
    "        self.out_layer = OutLayer(dim*edge_set_num, node_num, out_layer_num, inter_num = out_layer_inter_dim)\n",
    "        self.cache_edge_index_sets = [None] * edge_set_num\n",
    "        self.cache_embed_index = None\n",
    "        self.dp = nn.Dropout(0.2)\n",
    "        self.init_params()\n",
    "    \n",
    "    def init_params(self):\n",
    "        nn.init.kaiming_uniform_(self.embedding.weight, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, data, org_edge_index):\n",
    "        x = data.clone().detach()\n",
    "        edge_index_sets = self.edge_index_sets\n",
    "        device = data.device\n",
    "        batch_num, node_num, all_feature = x.shape\n",
    "        x = x.view(-1, all_feature).contiguous()\n",
    "        gcn_outs = []\n",
    "        for i, edge_index in enumerate(edge_index_sets):\n",
    "            edge_num = edge_index.shape[1]\n",
    "            cache_edge_index = self.cache_edge_index_sets[i]\n",
    "\n",
    "            if cache_edge_index is None or cache_edge_index.shape[1] != edge_num*batch_num:\n",
    "                self.cache_edge_index_sets[i] = get_batch_edge_index(edge_index, batch_num, node_num).to(device)\n",
    "            \n",
    "            batch_edge_index = self.cache_edge_index_sets[i]\n",
    "            all_embeddings = self.embedding(torch.arange(node_num).to(device))\n",
    "            weights_arr = all_embeddings.detach().clone()\n",
    "            all_embeddings = all_embeddings.repeat(batch_num, 1)\n",
    "            weights = weights_arr.view(node_num, -1)\n",
    "            cos_ji_mat = torch.matmul(weights, weights.T)\n",
    "            normed_mat = torch.matmul(weights.norm(dim=-1).view(-1,1), weights.norm(dim=-1).view(1,-1))\n",
    "            cos_ji_mat = cos_ji_mat / normed_mat\n",
    "            dim = weights.shape[-1]\n",
    "            topk_num = self.topk\n",
    "            topk_indices_ji = torch.topk(cos_ji_mat, topk_num, dim=-1)[1]\n",
    "            self.learned_graph = topk_indices_ji\n",
    "            gated_i = torch.arange(0, node_num).T.unsqueeze(1).repeat(1, topk_num).flatten().to(device).unsqueeze(0)\n",
    "            gated_j = topk_indices_ji.flatten().unsqueeze(0)\n",
    "            gated_edge_index = torch.cat((gated_j, gated_i), dim=0)\n",
    "            batch_gated_edge_index = get_batch_edge_index(gated_edge_index, batch_num, node_num).to(device)\n",
    "            gcn_out = self.gnn_layers[i](x, batch_gated_edge_index, node_num=node_num*batch_num, embedding=all_embeddings)\n",
    "            gcn_outs.append(gcn_out)\n",
    "\n",
    "        x = torch.cat(gcn_outs, dim=1)\n",
    "        x = x.view(batch_num, node_num, -1)\n",
    "        \n",
    "        indexes = torch.arange(0,node_num).to(device)\n",
    "        out = torch.mul(x, self.embedding(indexes))\n",
    "        \n",
    "        out = out.permute(0,2,1)\n",
    "        out = F.relu(self.bn_outlayer_in(out))\n",
    "        out = out.permute(0,2,1)\n",
    "\n",
    "        out = self.dp(out)\n",
    "        out = self.out_layer(out)\n",
    "        out = out.view(-1, node_num)\n",
    "        return out\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sDrVknvH0B7y"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_func(y_pred, y_true):\n",
    "    loss = F.mse_loss(y_pred, y_true, reduction='mean')\n",
    "    return loss\n",
    "\n",
    "def train(model = None, save_path = '', config={},  train_dataloader=None, val_dataloader=None, feature_map={}, test_dataloader=None, test_dataset=None, dataset_name='swat', train_dataset=None):\n",
    "    seed = config['seed']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=config['decay'])\n",
    "    now = time.time()\n",
    "    train_loss_list = []\n",
    "    cmp_loss_list = []\n",
    "    device = get_device()\n",
    "    acu_loss = 0\n",
    "    min_loss = 1e+8\n",
    "    min_f1 = 0\n",
    "    min_pre = 0\n",
    "    best_prec = 0\n",
    "\n",
    "    i = 0\n",
    "    epoch = config['epoch']\n",
    "    early_stop_win = 15\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    log_interval = 1000\n",
    "    stop_improve_count = 0\n",
    "\n",
    "    dataloader = train_dataloader\n",
    "\n",
    "    for i_epoch in range(epoch):\n",
    "        acu_loss = 0\n",
    "        model.train()\n",
    "        for x, labels, attack_labels, edge_index in dataloader:\n",
    "            _start = time.time()\n",
    "            x, labels, edge_index = [item.float().to(device) for item in [x, labels, edge_index]]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x, edge_index).float().to(device)\n",
    "            loss = loss_func(out, labels)            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_list.append(loss.item())\n",
    "            acu_loss += loss.item()\n",
    "            i += 1\n",
    "\n",
    "        # each epoch\n",
    "        print('epoch ({} / {}) (Loss:{:.8f}, ACU_loss:{:.8f})'.format(i_epoch, epoch, acu_loss/len(dataloader), acu_loss), flush=True)\n",
    "        # use val dataset to judge\n",
    "        if val_dataloader is not None:\n",
    "\n",
    "            val_loss, val_result = test(model, val_dataloader)\n",
    "\n",
    "            if val_loss < min_loss:\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                min_loss = val_loss\n",
    "                stop_improve_count = 0\n",
    "            else:\n",
    "                stop_improve_count += 1\n",
    "\n",
    "            if stop_improve_count >= early_stop_win:\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            if acu_loss < min_loss :\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                min_loss = acu_loss\n",
    "\n",
    "    return train_loss_list\n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "    # test\n",
    "    loss_func = nn.MSELoss(reduction='mean')\n",
    "    device = get_device()\n",
    "\n",
    "    test_loss_list = []\n",
    "    now = time.time()\n",
    "\n",
    "    test_predicted_list = []\n",
    "    test_ground_list = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    t_test_predicted_list = []\n",
    "    t_test_ground_list = []\n",
    "    t_test_labels_list = []\n",
    "\n",
    "    test_len = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    acu_loss = 0\n",
    "    for x, y, labels, edge_index in dataloader:\n",
    "        x, y, labels, edge_index = [item.to(device).float() for item in [x, y, labels, edge_index]]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted = model(x, edge_index).float().to(device)\n",
    "            loss = loss_func(predicted, y)\n",
    "            labels = labels.unsqueeze(1).repeat(1, predicted.shape[1])\n",
    "\n",
    "            if len(t_test_predicted_list) <= 0:\n",
    "                t_test_predicted_list = predicted\n",
    "                t_test_ground_list = y\n",
    "                t_test_labels_list = labels\n",
    "            else:\n",
    "                t_test_predicted_list = torch.cat((t_test_predicted_list, predicted), dim=0)\n",
    "                t_test_ground_list = torch.cat((t_test_ground_list, y), dim=0)\n",
    "                t_test_labels_list = torch.cat((t_test_labels_list, labels), dim=0)\n",
    "        \n",
    "        test_loss_list.append(loss.item())\n",
    "        acu_loss += loss.item()\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "        if i % 10000 == 1 and i > 1:\n",
    "            print(timeSincePlus(now, i / test_len))\n",
    "\n",
    "\n",
    "    test_predicted_list = t_test_predicted_list.tolist()        \n",
    "    test_ground_list = t_test_ground_list.tolist()        \n",
    "    test_labels_list = t_test_labels_list.tolist()      \n",
    "    \n",
    "    avg_loss = sum(test_loss_list)/len(test_loss_list)\n",
    "\n",
    "    return avg_loss, [test_predicted_list, test_ground_list, test_labels_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWvXrC560r8W"
   },
   "source": [
    "### Main Function\n",
    "<a id = \"driver\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CL1X0U1TiZB",
    "outputId": "45104264-6d74-4a73-84b2-fc1c4f044533"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch (0 / 1) (Loss:0.02150088, ACU_loss:41.02367432)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: './pretrained/saved_models/best_05|28-22:03:20.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 159>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    201\u001b[0m env_config\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_path\u001b[39m\u001b[38;5;124m'\u001b[39m: save_path_pattern,\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport\u001b[39m\u001b[38;5;124m'\u001b[39m: report,\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device,\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload_model_path\u001b[39m\u001b[38;5;124m'\u001b[39m: load_model_path\n\u001b[0;32m    206\u001b[0m }\n\u001b[0;32m    209\u001b[0m main \u001b[38;5;241m=\u001b[39m Main(train_config, env_config, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 210\u001b[0m \u001b[43mmain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mMain.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m     model_save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_save_path()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_log \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m# test            \u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03mself.model.load_state_dict(torch.load(model_save_path))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03mself.get_score(self.test_result, self.val_result)\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, save_path, config, train_dataloader, val_dataloader, feature_map, test_dataloader, test_dataset, dataset_name, train_dataset)\u001b[0m\n\u001b[0;32m     49\u001b[0m val_loss, val_result \u001b[38;5;241m=\u001b[39m test(model, val_dataloader)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m min_loss:\n\u001b[1;32m---> 52\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m     min_loss \u001b[38;5;241m=\u001b[39m val_loss\n\u001b[0;32m     54\u001b[0m     stop_improve_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\serialization.py:377\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    375\u001b[0m _check_dill_version(pickle_module)\n\u001b[1;32m--> 377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    379\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32md:\\miniconda\\lib\\site-packages\\torch\\serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: './pretrained/saved_models/best_05|28-22:03:20.pt'"
     ]
    }
   ],
   "source": [
    "class Main(object):\n",
    "    def __init__(self, train_config, env_config, debug=False):\n",
    "        self.train_config = train_config\n",
    "        self.env_config = env_config\n",
    "        self.datestr = None\n",
    "        train_orig = pd.read_csv('D:/papers/graphs/GDN/GDN/data/wadi/train.csv', sep=',', index_col=0)\n",
    "        test_orig = pd.read_csv('D:/papers/graphs/GDN/GDN/data/wadi/test.csv', sep=',', index_col=0)\n",
    "        feature_list_path = 'D:/papers/graphs/GDN/GDN/data/wadi/list.txt'\n",
    "       \n",
    "        train, test = train_orig, test_orig\n",
    "\n",
    "        if 'attack' in train.columns:\n",
    "            train = train.drop(columns=['attack'])\n",
    "\n",
    "        feature_map = get_feature_map(feature_list_path)\n",
    "        fc_struc = get_fc_graph_struc(feature_list_path)\n",
    "\n",
    "        set_device(env_config['device'])\n",
    "        self.device = get_device()\n",
    "\n",
    "        fc_edge_index = build_loc_net(fc_struc, list(train.columns), feature_map=feature_map)\n",
    "        fc_edge_index = torch.tensor(fc_edge_index, dtype = torch.long)\n",
    "\n",
    "        self.feature_map = feature_map\n",
    "\n",
    "        train_dataset_indata = construct_data(train, feature_map, labels=0)\n",
    "        test_dataset_indata = construct_data(test, feature_map, labels=test.attack.tolist())\n",
    "\n",
    "\n",
    "        cfg = {\n",
    "            'slide_win': train_config['slide_win'],\n",
    "            'slide_stride': train_config['slide_stride'],\n",
    "        }\n",
    "\n",
    "        train_dataset = TimeDataset(train_dataset_indata, fc_edge_index, mode='train', config=cfg)\n",
    "        test_dataset = TimeDataset(test_dataset_indata, fc_edge_index, mode='test', config=cfg)\n",
    "\n",
    "\n",
    "        train_dataloader, val_dataloader = self.get_loaders(train_dataset, train_config['seed'], train_config['batch'], val_ratio = train_config['val_ratio'])\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.test_dataloader = DataLoader(test_dataset, batch_size=train_config['batch'],\n",
    "                            shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "        edge_index_sets = []\n",
    "        edge_index_sets.append(fc_edge_index)\n",
    "\n",
    "        self.model = GDN(edge_index_sets, len(feature_map), \n",
    "                dim=train_config['dim'], \n",
    "                input_dim=train_config['slide_win'],\n",
    "                out_layer_num=train_config['out_layer_num'],\n",
    "                out_layer_inter_dim=train_config['out_layer_inter_dim'],\n",
    "                topk=train_config['topk']\n",
    "            ).to(self.device)\n",
    "\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        if len(self.env_config['load_model_path']) > 0:\n",
    "            model_save_path = self.env_config['load_model_path']\n",
    "        else:\n",
    "            model_save_path = self.get_save_path()[0]\n",
    "\n",
    "            self.train_log = train(self.model, model_save_path, \n",
    "                config = train_config,\n",
    "                train_dataloader=self.train_dataloader,\n",
    "                val_dataloader=self.val_dataloader, \n",
    "                feature_map=self.feature_map,\n",
    "                test_dataloader=self.test_dataloader,\n",
    "                test_dataset=self.test_dataset,\n",
    "                train_dataset=self.train_dataset,\n",
    "            )\n",
    "        \"\"\"\n",
    "        # test            \n",
    "        self.model.load_state_dict(torch.load(model_save_path))\n",
    "        best_model = self.model.to(self.device)\n",
    "\n",
    "        _, self.test_result = test(best_model, self.test_dataloader)\n",
    "        _, self.val_result = test(best_model, self.val_dataloader)\n",
    "\n",
    "        self.get_score(self.test_result, self.val_result)\n",
    "        \"\"\"\n",
    "\n",
    "    def get_loaders(self, train_dataset, seed, batch, val_ratio=0.1):\n",
    "        dataset_len = int(len(train_dataset))\n",
    "        train_use_len = int(dataset_len * (1 - val_ratio))\n",
    "        val_use_len = int(dataset_len * val_ratio)\n",
    "        val_start_index = random.randrange(train_use_len)\n",
    "        indices = torch.arange(dataset_len)\n",
    "\n",
    "        train_sub_indices = torch.cat([indices[:val_start_index], indices[val_start_index+val_use_len:]])\n",
    "        train_subset = Subset(train_dataset, train_sub_indices)\n",
    "\n",
    "        val_sub_indices = indices[val_start_index:val_start_index+val_use_len]\n",
    "        val_subset = Subset(train_dataset, val_sub_indices)\n",
    "\n",
    "\n",
    "        train_dataloader = DataLoader(train_subset, batch_size=batch,\n",
    "                                shuffle=True)\n",
    "\n",
    "        val_dataloader = DataLoader(val_subset, batch_size=batch,\n",
    "                                shuffle=False)\n",
    "\n",
    "        return train_dataloader, val_dataloader\n",
    "\n",
    "    def get_score(self, test_result, val_result):\n",
    "\n",
    "        feature_num = len(test_result[0][0])\n",
    "        np_test_result = np.array(test_result)\n",
    "        np_val_result = np.array(val_result)\n",
    "\n",
    "        test_labels = np_test_result[2, :, 0].tolist()\n",
    "    \n",
    "        test_scores, normal_scores = get_full_err_scores(test_result, val_result)\n",
    "\n",
    "        top1_best_info = get_best_performance_data(test_scores, test_labels, topk=1) \n",
    "        top1_val_info = get_val_performance_data(test_scores, normal_scores, test_labels, topk=1)\n",
    "\n",
    "\n",
    "        print('=========================** Result **============================\\n')\n",
    "\n",
    "        info = None\n",
    "        if self.env_config['report'] == 'best':\n",
    "            info = top1_best_info\n",
    "        elif self.env_config['report'] == 'val':\n",
    "            info = top1_val_info\n",
    "\n",
    "        print(f'F1 score: {info[0]}')\n",
    "        print(f'precision: {info[1]}')\n",
    "        print(f'recall: {info[2]}\\n')\n",
    "\n",
    "\n",
    "    def get_save_path(self, feature_name=''):\n",
    "\n",
    "        dir_path = self.env_config['save_path']\n",
    "        \n",
    "        if self.datestr is None:\n",
    "            now = datetime.now()\n",
    "            self.datestr = now.strftime('%m|%d-%H:%M:%S')\n",
    "        datestr = self.datestr          \n",
    "\n",
    "        paths = [\n",
    "            f'./pretrained/{dir_path}/best_{datestr}.pt',\n",
    "            f'./results/{dir_path}/{datestr}.csv',\n",
    "        ]\n",
    "\n",
    "        for path in paths:\n",
    "            dirname = os.path.dirname(path)\n",
    "            Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        return paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    batch = 32\n",
    "    epoch = 1\n",
    "    slide_win = 5\n",
    "    dim = 64\n",
    "    slide_stride = 1\n",
    "    save_path_pattern = \"saved_models\"\n",
    "    device = 'cpu'\n",
    "    random_seed = 5\n",
    "    out_layer_num = 1\n",
    "    out_layer_inter_dim = 128\n",
    "    decay = 0\n",
    "    val_ratio = 0.2\n",
    "    topk = 5\n",
    "    report = 'best'\n",
    "    load_model_path = ''\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.environ['PYTHONHASHSEED'] = str(random_seed)\n",
    "\n",
    "\n",
    "    train_config = {\n",
    "        'batch': batch,\n",
    "        'epoch': epoch,\n",
    "        'slide_win': slide_win,\n",
    "        'dim': dim,\n",
    "        'slide_stride': slide_stride,\n",
    "        'seed': random_seed,\n",
    "        'out_layer_num': out_layer_num,\n",
    "        'out_layer_inter_dim': out_layer_inter_dim,\n",
    "        'decay': decay,\n",
    "        'val_ratio': val_ratio,\n",
    "        'topk': topk,\n",
    "    }\n",
    "\n",
    "    env_config={\n",
    "        'save_path': save_path_pattern,\n",
    "        'report': report,\n",
    "        'device': device,\n",
    "        'load_model_path': load_model_path\n",
    "    }\n",
    "    \n",
    "\n",
    "    main = Main(train_config, env_config, debug=False)\n",
    "    main.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "GDN_with_doc.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
