{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from utils import process\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        c_x = torch.unsqueeze(c, 1)\n",
    "        c_x = c_x.expand_as(h_pl)\n",
    "\n",
    "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
    "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
    "\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "\n",
    "        logits = torch.cat((sc_1, sc_2), 1)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator2(nn.Module):\n",
    "    def __init__(self, n_h):\n",
    "        super(Discriminator2, self).__init__()\n",
    "        self.f_k = nn.Bilinear(n_h, n_h, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Bilinear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n",
    "        # c_x = torch.unsqueeze(c, 1)\n",
    "        # c_x = c_x.expand_as(h_pl)\n",
    "        c_x = c\n",
    "        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n",
    "        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n",
    "\n",
    "        if s_bias1 is not None:\n",
    "            sc_1 += s_bias1\n",
    "        if s_bias2 is not None:\n",
    "            sc_2 += s_bias2\n",
    "\n",
    "        logits = torch.cat((sc_1, sc_2), 1)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_ft, out_ft, act, bias=True):\n",
    "        super(GCN, self).__init__()\n",
    "        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n",
    "        self.act = nn.PReLU() if act == 'prelu' else act\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n",
    "            self.bias.data.fill_(0.0)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    # Shape of seq: (batch, nodes, features)\n",
    "    def forward(self, seq, adj, sparse=False):\n",
    "        seq_fts = self.fc(seq)\n",
    "        if sparse:\n",
    "            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n",
    "        else:\n",
    "            out = torch.bmm(adj, seq_fts)\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "        \n",
    "        return self.act(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies an average on seq, of shape (batch, nodes, features)\n",
    "# While taking into account the masking of msk\n",
    "class AvgReadout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AvgReadout, self).__init__()\n",
    "\n",
    "    def forward(self, seq, msk):\n",
    "        if msk is None:\n",
    "            return torch.mean(seq, 1)\n",
    "        else:\n",
    "            msk = torch.unsqueeze(msk, -1)\n",
    "            return torch.sum(seq * msk, 1) / torch.sum(msk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(nn.Module):\n",
    "    def __init__(self, ft_in, nb_classes):\n",
    "        super(LogReg, self).__init__()\n",
    "        self.fc = nn.Linear(ft_in, nb_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            self.weights_init(m)\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        ret = self.fc(seq)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class DGI(nn.Module):\n",
    "    def __init__(self, n_in, n_h, activation):\n",
    "        super(DGI, self).__init__()\n",
    "        self.gcn = GCN(n_in, n_h, activation)\n",
    "        self.read = AvgReadout()\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.disc = Discriminator(n_h)\n",
    "        self.disc2 = Discriminator2(n_h)\n",
    "\n",
    "    def forward(self, seq1, seq2, seq3, seq4, adj, aug_adj1, aug_adj2, sparse, msk, samp_bias1, samp_bias2, aug_type):\n",
    "        \n",
    "        h_0 = self.gcn(seq1, adj, sparse)\n",
    "\n",
    "        h_1 = self.gcn(seq3, aug_adj1, sparse)\n",
    "        h_3 = self.gcn(seq4, aug_adj2, sparse)\n",
    "             \n",
    "        c_1 = self.read(h_1, msk)\n",
    "        c_1= self.sigm(c_1)\n",
    "\n",
    "        c_3 = self.read(h_3, msk)\n",
    "        c_3= self.sigm(c_3)\n",
    "\n",
    "        h_2 = self.gcn(seq2, adj, sparse)\n",
    "\n",
    "        ret1 = self.disc(c_1, h_0, h_2, samp_bias1, samp_bias2)\n",
    "        ret2 = self.disc(c_3, h_0, h_2, samp_bias1, samp_bias2)\n",
    "\n",
    "        ret = ret1 + ret2\n",
    "        return ret\n",
    "\n",
    "    # Detach the return variables\n",
    "    def embed(self, seq, adj, sparse, msk):\n",
    "        h_1 = self.gcn(seq, adj, sparse)\n",
    "        c = self.read(h_1, msk)\n",
    "\n",
    "        return h_1.detach(), c.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_row_col(input_matrix, drop_list, only_row=False):\n",
    "\n",
    "    remain_list = [i for i in range(input_matrix.shape[0]) if i not in drop_list]\n",
    "    out = input_matrix[remain_list, :]\n",
    "    if only_row:\n",
    "        return out\n",
    "    out = out[:, remain_list]\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def aug_subgraph(input_fea, input_adj, drop_percent=0.2):\n",
    "    input_adj = torch.tensor(input_adj.todense().tolist())\n",
    "    input_fea = input_fea.squeeze(0)\n",
    "    node_num = input_fea.shape[0]\n",
    "\n",
    "    all_node_list = [i for i in range(node_num)]\n",
    "    s_node_num = int(node_num * (1 - drop_percent))\n",
    "    center_node_id = random.randint(0, node_num - 1)\n",
    "    sub_node_id_list = [center_node_id]\n",
    "    all_neighbor_list = []\n",
    "\n",
    "    for i in range(s_node_num - 1):\n",
    "        \n",
    "        all_neighbor_list += torch.nonzero(input_adj[sub_node_id_list[i]], as_tuple=False).squeeze(1).tolist()\n",
    "        \n",
    "        all_neighbor_list = list(set(all_neighbor_list))\n",
    "        new_neighbor_list = [n for n in all_neighbor_list if not n in sub_node_id_list]\n",
    "        if len(new_neighbor_list) != 0:\n",
    "            new_node = random.sample(new_neighbor_list, 1)[0]\n",
    "            sub_node_id_list.append(new_node)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    \n",
    "    drop_node_list = sorted([i for i in all_node_list if not i in sub_node_id_list])\n",
    "\n",
    "    aug_input_fea = delete_row_col(input_fea, drop_node_list, only_row=True)\n",
    "    aug_input_adj = delete_row_col(input_adj, drop_node_list)\n",
    "\n",
    "    aug_input_fea = aug_input_fea.unsqueeze(0)\n",
    "    aug_input_adj = sp.csr_matrix(np.matrix(aug_input_adj))\n",
    "\n",
    "    return aug_input_fea, aug_input_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the features is: (2708, 1433)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset = \"cora\"\n",
    "aug_type = \"subgraph\"\n",
    "drop_percent = 0.20\n",
    "save_name = \"cora_best_dgi\"\n",
    "\n",
    "# training params\n",
    "batch_size = 1\n",
    "nb_epochs = 10000\n",
    "patience = 20\n",
    "lr = 0.001\n",
    "l2_coef = 0.0\n",
    "drop_prob = 0.0\n",
    "hid_units = 512\n",
    "sparse = True\n",
    "\n",
    "nonlinearity = 'prelu' # special name to separate parameters\n",
    "adj, features, labels, idx_train, idx_val, idx_test = process.load_data(dataset)\n",
    "features, _ = process.preprocess_features(features)\n",
    "\n",
    "print(\"Shape of the features is: {}\".format(features.shape))\n",
    "nb_nodes = features.shape[0]  # node number\n",
    "ft_size = features.shape[1]   # node features dim\n",
    "nb_classes = labels.shape[1]  # classes = 6\n",
    "\n",
    "features = torch.FloatTensor(features[np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Augmentations!\n",
      "Loss:[0.6931]\n",
      "Loss:[0.7016]\n",
      "Loss:[0.6857]\n",
      "Loss:[0.6906]\n",
      "Loss:[0.6875]\n",
      "Loss:[0.6795]\n",
      "Loss:[0.6759]\n",
      "Loss:[0.6758]\n",
      "Loss:[0.6667]\n",
      "Loss:[0.6591]\n",
      "Loss:[0.6560]\n",
      "Loss:[0.6444]\n",
      "Loss:[0.6359]\n",
      "Loss:[0.6294]\n",
      "Loss:[0.6152]\n",
      "Loss:[0.6084]\n",
      "Loss:[0.5931]\n",
      "Loss:[0.5844]\n",
      "Loss:[0.5646]\n",
      "Loss:[0.5573]\n",
      "Loss:[0.5392]\n",
      "Loss:[0.5239]\n",
      "Loss:[0.5130]\n",
      "Loss:[0.4974]\n",
      "Loss:[0.4802]\n",
      "Loss:[0.4616]\n",
      "Loss:[0.4552]\n",
      "Loss:[0.4391]\n",
      "Loss:[0.4206]\n",
      "Loss:[0.4111]\n",
      "Loss:[0.3957]\n",
      "Loss:[0.3797]\n",
      "Loss:[0.3643]\n",
      "Loss:[0.3667]\n",
      "Loss:[0.3465]\n",
      "Loss:[0.3412]\n",
      "Loss:[0.3242]\n",
      "Loss:[0.3150]\n",
      "Loss:[0.3016]\n",
      "Loss:[0.2947]\n",
      "Loss:[0.2864]\n",
      "Loss:[0.2820]\n",
      "Loss:[0.2768]\n",
      "Loss:[0.2558]\n",
      "Loss:[0.2623]\n",
      "Loss:[0.2540]\n",
      "Loss:[0.2498]\n",
      "Loss:[0.2373]\n",
      "Loss:[0.2405]\n",
      "Loss:[0.2103]\n",
      "Loss:[0.2211]\n",
      "Loss:[0.2362]\n",
      "Loss:[0.2129]\n",
      "Loss:[0.1987]\n",
      "Loss:[0.2026]\n",
      "Loss:[0.2042]\n",
      "Loss:[0.1904]\n",
      "Loss:[0.1952]\n",
      "Loss:[0.1986]\n",
      "Loss:[0.1828]\n",
      "Loss:[0.1846]\n",
      "Loss:[0.1846]\n",
      "Loss:[0.1707]\n",
      "Loss:[0.1648]\n",
      "Loss:[0.1739]\n",
      "Loss:[0.1705]\n",
      "Loss:[0.1670]\n",
      "Loss:[0.1617]\n",
      "Loss:[0.1533]\n",
      "Loss:[0.1635]\n",
      "Loss:[0.1567]\n",
      "Loss:[0.1449]\n",
      "Loss:[0.1424]\n",
      "Loss:[0.1365]\n",
      "Loss:[0.1532]\n",
      "Loss:[0.1450]\n",
      "Loss:[0.1448]\n",
      "Loss:[0.1373]\n",
      "Loss:[0.1329]\n",
      "Loss:[0.1435]\n",
      "Loss:[0.1352]\n",
      "Loss:[0.1227]\n",
      "Loss:[0.1355]\n",
      "Loss:[0.1307]\n",
      "Loss:[0.1243]\n",
      "Loss:[0.1208]\n",
      "Loss:[0.1216]\n",
      "Loss:[0.1208]\n",
      "Loss:[0.1287]\n",
      "Loss:[0.1225]\n",
      "Loss:[0.1211]\n",
      "Loss:[0.1189]\n",
      "Loss:[0.1042]\n",
      "Loss:[0.1177]\n",
      "Loss:[0.1066]\n",
      "Loss:[0.1102]\n",
      "Loss:[0.1237]\n",
      "Loss:[0.1075]\n",
      "Loss:[0.1113]\n",
      "Loss:[0.0960]\n",
      "Loss:[0.1078]\n",
      "Loss:[0.1042]\n",
      "Loss:[0.1044]\n",
      "Loss:[0.0943]\n",
      "Loss:[0.1021]\n",
      "Loss:[0.1083]\n",
      "Loss:[0.0936]\n",
      "Loss:[0.0932]\n",
      "Loss:[0.0924]\n",
      "Loss:[0.1036]\n",
      "Loss:[0.0956]\n",
      "Loss:[0.1048]\n",
      "Loss:[0.0853]\n",
      "Loss:[0.0917]\n",
      "Loss:[0.0940]\n",
      "Loss:[0.0973]\n",
      "Loss:[0.0833]\n",
      "Loss:[0.0816]\n",
      "Loss:[0.0840]\n",
      "Loss:[0.0886]\n",
      "Loss:[0.0826]\n",
      "Loss:[0.0793]\n",
      "Loss:[0.0918]\n",
      "Loss:[0.0917]\n",
      "Loss:[0.0758]\n",
      "Loss:[0.0940]\n",
      "Loss:[0.0814]\n",
      "Loss:[0.0799]\n",
      "Loss:[0.0902]\n",
      "Loss:[0.0887]\n",
      "Loss:[0.0854]\n",
      "Loss:[0.0754]\n",
      "Loss:[0.0965]\n",
      "Loss:[0.0944]\n",
      "Loss:[0.0818]\n",
      "Loss:[0.0971]\n",
      "Loss:[0.0779]\n",
      "Loss:[0.0880]\n",
      "Loss:[0.0880]\n",
      "Loss:[0.0791]\n",
      "Loss:[0.0922]\n",
      "Loss:[0.0708]\n",
      "Loss:[0.0769]\n",
      "Loss:[0.0829]\n",
      "Loss:[0.0744]\n",
      "Loss:[0.0768]\n",
      "Loss:[0.0676]\n",
      "Loss:[0.0700]\n",
      "Loss:[0.0663]\n",
      "Loss:[0.0734]\n",
      "Loss:[0.0603]\n",
      "Loss:[0.0690]\n",
      "Loss:[0.0626]\n",
      "Loss:[0.0715]\n",
      "Loss:[0.0690]\n",
      "Loss:[0.0740]\n",
      "Loss:[0.0619]\n",
      "Loss:[0.0652]\n",
      "Loss:[0.0609]\n",
      "Loss:[0.0668]\n",
      "Loss:[0.0622]\n",
      "Loss:[0.0649]\n",
      "Loss:[0.0610]\n",
      "Loss:[0.0581]\n",
      "Loss:[0.0635]\n",
      "Loss:[0.0662]\n",
      "Loss:[0.0611]\n",
      "Loss:[0.0582]\n",
      "Loss:[0.0528]\n",
      "Loss:[0.0523]\n",
      "Loss:[0.0585]\n",
      "Loss:[0.0659]\n",
      "Loss:[0.0609]\n",
      "Loss:[0.0652]\n",
      "Loss:[0.0601]\n",
      "Loss:[0.0567]\n",
      "Loss:[0.0474]\n",
      "Loss:[0.0567]\n",
      "Loss:[0.0482]\n",
      "Loss:[0.0490]\n",
      "Loss:[0.0517]\n",
      "Loss:[0.0495]\n",
      "Loss:[0.0555]\n",
      "Loss:[0.0547]\n",
      "Loss:[0.0451]\n",
      "Loss:[0.0544]\n",
      "Loss:[0.0530]\n",
      "Loss:[0.0503]\n",
      "Loss:[0.0434]\n",
      "Loss:[0.0458]\n",
      "Loss:[0.0529]\n",
      "Loss:[0.0489]\n",
      "Loss:[0.0497]\n",
      "Loss:[0.0481]\n",
      "Loss:[0.0426]\n",
      "Loss:[0.0413]\n",
      "Loss:[0.0388]\n",
      "Loss:[0.0510]\n",
      "Loss:[0.0467]\n",
      "Loss:[0.0414]\n",
      "Loss:[0.0450]\n",
      "Loss:[0.0434]\n",
      "Loss:[0.0437]\n",
      "Loss:[0.0416]\n",
      "Loss:[0.0408]\n",
      "Loss:[0.0483]\n",
      "Loss:[0.0433]\n",
      "Loss:[0.0427]\n",
      "Loss:[0.0497]\n",
      "Loss:[0.0428]\n",
      "Loss:[0.0467]\n",
      "Loss:[0.0412]\n",
      "Loss:[0.0453]\n",
      "Loss:[0.0418]\n",
      "Loss:[0.0438]\n",
      "Loss:[0.0419]\n",
      "Loss:[0.0379]\n",
      "Loss:[0.0459]\n",
      "Loss:[0.0415]\n",
      "Loss:[0.0377]\n",
      "Loss:[0.0454]\n",
      "Loss:[0.0408]\n",
      "Loss:[0.0406]\n",
      "Loss:[0.0311]\n",
      "Loss:[0.0527]\n",
      "Loss:[0.0384]\n",
      "Loss:[0.0388]\n",
      "Loss:[0.0384]\n",
      "Loss:[0.0434]\n",
      "Loss:[0.0341]\n",
      "Loss:[0.0381]\n",
      "Loss:[0.0388]\n",
      "Loss:[0.0349]\n",
      "Loss:[0.0342]\n",
      "Loss:[0.0415]\n",
      "Loss:[0.0417]\n",
      "Loss:[0.0333]\n",
      "Loss:[0.0402]\n",
      "Loss:[0.0392]\n",
      "Loss:[0.0332]\n",
      "Loss:[0.0390]\n",
      "Loss:[0.0318]\n",
      "Loss:[0.0315]\n",
      "Loss:[0.0363]\n",
      "Early stopping!\n",
      "Loading 223th epoch\n",
      "acc:[0.8050]\n",
      "acc:[0.8090]\n",
      "acc:[0.8100]\n",
      "acc:[0.8090]\n",
      "acc:[0.8060]\n",
      "acc:[0.8060]\n",
      "acc:[0.8060]\n",
      "acc:[0.8070]\n",
      "acc:[0.8060]\n",
      "acc:[0.8050]\n",
      "acc:[0.8090]\n",
      "acc:[0.8100]\n",
      "acc:[0.8080]\n",
      "acc:[0.8050]\n",
      "acc:[0.8070]\n",
      "acc:[0.8040]\n",
      "acc:[0.8070]\n",
      "acc:[0.8070]\n",
      "acc:[0.8110]\n",
      "acc:[0.8030]\n",
      "acc:[0.8070]\n",
      "acc:[0.8070]\n",
      "acc:[0.8040]\n",
      "acc:[0.8100]\n",
      "acc:[0.8080]\n",
      "acc:[0.8050]\n",
      "acc:[0.8100]\n",
      "acc:[0.8080]\n",
      "acc:[0.8060]\n",
      "acc:[0.8080]\n",
      "acc:[0.8070]\n",
      "acc:[0.8060]\n",
      "acc:[0.8100]\n",
      "acc:[0.8070]\n",
      "acc:[0.8090]\n",
      "acc:[0.8090]\n",
      "acc:[0.8060]\n",
      "acc:[0.8090]\n",
      "acc:[0.8070]\n",
      "acc:[0.8110]\n",
      "acc:[0.8080]\n",
      "acc:[0.8090]\n",
      "acc:[0.8050]\n",
      "acc:[0.8080]\n",
      "acc:[0.8060]\n",
      "acc:[0.8070]\n",
      "acc:[0.8060]\n",
      "acc:[0.8080]\n",
      "acc:[0.8080]\n",
      "acc:[0.8080]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Average accuracy:[0.8073]\n",
      "Mean:[80.7340]\n",
      "Std :[0.1869]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin Augmentations!\")\n",
    "\n",
    "aug_features1, aug_adj1 = aug_subgraph(features, adj, drop_percent=drop_percent)\n",
    "aug_features2, aug_adj2 = aug_subgraph(features, adj, drop_percent=drop_percent)\n",
    "\n",
    "\n",
    "adj = process.normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "aug_adj1 = process.normalize_adj(aug_adj1 + sp.eye(aug_adj1.shape[0]))\n",
    "aug_adj2 = process.normalize_adj(aug_adj2 + sp.eye(aug_adj2.shape[0]))\n",
    "\n",
    "if sparse:\n",
    "    sp_adj = process.sparse_mx_to_torch_sparse_tensor(adj)\n",
    "    sp_aug_adj1 = process.sparse_mx_to_torch_sparse_tensor(aug_adj1)\n",
    "    sp_aug_adj2 = process.sparse_mx_to_torch_sparse_tensor(aug_adj2)\n",
    "\n",
    "else:\n",
    "    adj = (adj + sp.eye(adj.shape[0])).todense()\n",
    "    aug_adj1 = (aug_adj1 + sp.eye(aug_adj1.shape[0])).todense()\n",
    "    aug_adj2 = (aug_adj2 + sp.eye(aug_adj2.shape[0])).todense()\n",
    "\n",
    "\n",
    "# Mask\n",
    "if not sparse:\n",
    "    adj = torch.FloatTensor(adj[np.newaxis])\n",
    "    aug_adj1 = torch.FloatTensor(aug_adj1[np.newaxis])\n",
    "    aug_adj2 = torch.FloatTensor(aug_adj2[np.newaxis])\n",
    "\n",
    "\n",
    "labels = torch.FloatTensor(labels[np.newaxis])\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "model = DGI(ft_size, hid_units, nonlinearity).to(\"cuda\")\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_coef)\n",
    "\n",
    "\n",
    "# Doing it safely, incase CUDA is not available\n",
    "if torch.cuda.is_available():\n",
    "    features = features.cuda()\n",
    "    aug_features1 = aug_features1.cuda()\n",
    "    aug_features2 = aug_features2.cuda()\n",
    "    if sparse:\n",
    "        sp_adj = sp_adj.cuda()\n",
    "        sp_aug_adj1 = sp_aug_adj1.cuda()\n",
    "        sp_aug_adj2 = sp_aug_adj2.cuda()\n",
    "    else:\n",
    "        adj = adj.cuda()\n",
    "        aug_adj1 = aug_adj1.cuda()\n",
    "        aug_adj2 = aug_adj2.cuda()\n",
    "\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "b_xent = nn.BCEWithLogitsLoss()\n",
    "xent = nn.CrossEntropyLoss()\n",
    "cnt_wait = 0\n",
    "best = 1e9\n",
    "best_t = 0\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    model.train()\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    idx = np.random.permutation(nb_nodes)\n",
    "    shuf_fts = features[:, idx, :]\n",
    "\n",
    "    lbl_1 = torch.ones(batch_size, nb_nodes)\n",
    "    lbl_2 = torch.zeros(batch_size, nb_nodes)\n",
    "    lbl = torch.cat((lbl_1, lbl_2), 1)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        shuf_fts = shuf_fts.cuda()\n",
    "        lbl = lbl.cuda()\n",
    "    \n",
    "    logits = model(features, shuf_fts, aug_features1, aug_features2,\n",
    "                   sp_adj if sparse else adj, \n",
    "                   sp_aug_adj1 if sparse else aug_adj1,\n",
    "                   sp_aug_adj2 if sparse else aug_adj2,  \n",
    "                   sparse, None, None, None, aug_type=aug_type) \n",
    "\n",
    "    loss = b_xent(logits, lbl)\n",
    "    print('Loss:[{:.4f}]'.format(loss.item()))\n",
    "\n",
    "    if loss < best:\n",
    "        best = loss\n",
    "        best_t = epoch\n",
    "        cnt_wait = 0\n",
    "        torch.save(model.state_dict(), save_name)\n",
    "    else:\n",
    "        cnt_wait += 1\n",
    "\n",
    "    if cnt_wait == patience:\n",
    "        print('Early stopping!')\n",
    "        break\n",
    "\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "\n",
    "print('Loading {}th epoch'.format(best_t))\n",
    "model.load_state_dict(torch.load(save_name))\n",
    "\n",
    "model = model.to('cuda')\n",
    "\n",
    "embeds, _ = model.embed(features, sp_adj if sparse else adj, sparse, None)\n",
    "train_embs = embeds[0, idx_train]\n",
    "val_embs = embeds[0, idx_val]\n",
    "test_embs = embeds[0, idx_test]\n",
    "\n",
    "train_lbls = torch.argmax(labels[0, idx_train], dim=1)\n",
    "val_lbls = torch.argmax(labels[0, idx_val], dim=1)\n",
    "test_lbls = torch.argmax(labels[0, idx_test], dim=1)\n",
    "\n",
    "tot = torch.zeros(1).to(\"cuda\")\n",
    "\n",
    "accs = []\n",
    "\n",
    "for _ in range(50):\n",
    "    log = LogReg(hid_units, nb_classes).to(\"cuda\")\n",
    "    opt = torch.optim.Adam(log.parameters(), lr=0.01, weight_decay=0.0)\n",
    "\n",
    "    pat_steps = 0\n",
    "    best_acc = torch.zeros(1)\n",
    "    best_acc = best_acc.cpu()\n",
    "    for _ in range(100):\n",
    "        log.train()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        logits = log(train_embs)\n",
    "        loss = xent(logits, train_lbls)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    logits = log(test_embs)\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = torch.sum(preds == test_lbls).float() / test_lbls.shape[0]\n",
    "    accs.append(acc * 100)\n",
    "    print('acc:[{:.4f}]'.format(acc))\n",
    "    tot += acc\n",
    "\n",
    "print('-' * 100)\n",
    "print('Average accuracy:[{:.4f}]'.format(tot.item() / 50))\n",
    "accs = torch.stack(accs)\n",
    "print('Mean:[{:.4f}]'.format(accs.mean().item()))\n",
    "print('Std :[{:.4f}]'.format(accs.std().item()))\n",
    "print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
