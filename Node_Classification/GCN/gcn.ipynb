{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import SparseTensor, fill_diag, matmul, mul\n",
    "from torch_sparse import sum as sparsesum\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.nn.inits import zeros\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "def gcn_norm(edge_index, edge_weight=None, num_nodes=None, improved=False, add_self_loops=True, flow=\"source_to_target\", dtype=None):\n",
    "    fill_value = 2. if improved else 1.\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    if edge_weight is None:\n",
    "        edge_weight = torch.ones((edge_index.size(1), ), dtype=dtype,\n",
    "                                 device=edge_index.device)\n",
    "\n",
    "    if add_self_loops:\n",
    "        edge_index, tmp_edge_weight = add_remaining_self_loops(edge_index, edge_weight, \n",
    "                                                               fill_value, num_nodes)\n",
    "\n",
    "        edge_weight = tmp_edge_weight\n",
    "\n",
    "    row, col = edge_index[0], edge_index[1]\n",
    "    idx = col if flow == \"source_to_target\" else row\n",
    "    deg = scatter_add(edge_weight, idx, dim=0, dim_size=num_nodes)\n",
    "    deg_inv_sqrt = deg.pow_(-0.5)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0)\n",
    "    return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels,**kwargs):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super().__init__(**kwargs)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = False\n",
    "        self.add_self_loops = True\n",
    "        self.normalize = True\n",
    "        \n",
    "        self.lin = Linear(in_channels, out_channels, bias=False, weight_initializer='glorot')\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        zeros(self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight = None):\n",
    "        if self.normalize:\n",
    "            edge_index, edge_weight = gcn_norm(edge_index, edge_weight, \n",
    "                                               x.size(self.node_dim), self.improved, \n",
    "                                               self.add_self_loops, self.flow)\n",
    "\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight, size=None)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_weight):\n",
    "        return x_j if edge_weight is None else edge_weight.view(-1, 1)*x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t, x):\n",
    "        return matmul(adj_t, x, reduce=self.aggr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL Loss: 1.9540339708328247\n",
      "NLL Loss: 1.8549575805664062\n",
      "NLL Loss: 1.722895860671997\n",
      "NLL Loss: 1.6031818389892578\n",
      "NLL Loss: 1.4577544927597046\n",
      "NLL Loss: 1.3632465600967407\n",
      "NLL Loss: 1.214414358139038\n",
      "NLL Loss: 1.1096059083938599\n",
      "NLL Loss: 1.0113153457641602\n",
      "NLL Loss: 0.9122030735015869\n",
      "NLL Loss: 0.8009372353553772\n",
      "NLL Loss: 0.738771378993988\n",
      "NLL Loss: 0.6319516897201538\n",
      "NLL Loss: 0.5524605512619019\n",
      "NLL Loss: 0.48144006729125977\n",
      "NLL Loss: 0.45087242126464844\n",
      "NLL Loss: 0.42074069380760193\n",
      "NLL Loss: 0.37666186690330505\n",
      "NLL Loss: 0.30514439940452576\n",
      "NLL Loss: 0.2766708433628082\n",
      "NLL Loss: 0.21188490092754364\n",
      "NLL Loss: 0.20924663543701172\n",
      "NLL Loss: 0.18866832554340363\n",
      "NLL Loss: 0.16344475746154785\n",
      "NLL Loss: 0.1463128626346588\n",
      "NLL Loss: 0.16747958958148956\n",
      "NLL Loss: 0.09954558312892914\n",
      "NLL Loss: 0.11196306347846985\n",
      "NLL Loss: 0.1226775050163269\n",
      "NLL Loss: 0.0986974909901619\n",
      "NLL Loss: 0.11168782413005829\n",
      "NLL Loss: 0.11779920011758804\n",
      "NLL Loss: 0.10296595096588135\n",
      "NLL Loss: 0.12333191186189651\n",
      "NLL Loss: 0.07639509439468384\n",
      "NLL Loss: 0.09177202731370926\n",
      "NLL Loss: 0.06810163706541061\n",
      "NLL Loss: 0.06792213022708893\n",
      "NLL Loss: 0.06717830896377563\n",
      "NLL Loss: 0.057850781828165054\n",
      "NLL Loss: 0.05545620992779732\n",
      "NLL Loss: 0.07419512420892715\n",
      "NLL Loss: 0.08494015783071518\n",
      "NLL Loss: 0.05636029690504074\n",
      "NLL Loss: 0.04598128795623779\n",
      "NLL Loss: 0.044356588274240494\n",
      "NLL Loss: 0.05236250162124634\n",
      "NLL Loss: 0.06830161809921265\n",
      "NLL Loss: 0.06562309712171555\n",
      "NLL Loss: 0.0738539844751358\n",
      "NLL Loss: 0.04349175840616226\n",
      "NLL Loss: 0.03837095573544502\n",
      "NLL Loss: 0.05772380158305168\n",
      "NLL Loss: 0.05965195968747139\n",
      "NLL Loss: 0.03779923543334007\n",
      "NLL Loss: 0.04328646510839462\n",
      "NLL Loss: 0.04195668175816536\n",
      "NLL Loss: 0.0393793098628521\n",
      "NLL Loss: 0.034031182527542114\n",
      "NLL Loss: 0.053794946521520615\n",
      "NLL Loss: 0.040658704936504364\n",
      "NLL Loss: 0.04229780286550522\n",
      "NLL Loss: 0.04169927164912224\n",
      "NLL Loss: 0.04031902551651001\n",
      "NLL Loss: 0.043038468807935715\n",
      "NLL Loss: 0.04008239880204201\n",
      "NLL Loss: 0.02866363152861595\n",
      "NLL Loss: 0.03493635356426239\n",
      "NLL Loss: 0.045855890959501266\n",
      "NLL Loss: 0.054305534809827805\n",
      "NLL Loss: 0.050097882747650146\n",
      "NLL Loss: 0.04325484484434128\n",
      "NLL Loss: 0.044114381074905396\n",
      "NLL Loss: 0.04234214127063751\n",
      "NLL Loss: 0.055313389748334885\n",
      "NLL Loss: 0.05792199447751045\n",
      "NLL Loss: 0.05160826817154884\n",
      "NLL Loss: 0.04612685367465019\n",
      "NLL Loss: 0.040598880499601364\n",
      "NLL Loss: 0.028933413326740265\n",
      "NLL Loss: 0.0375836119055748\n",
      "NLL Loss: 0.0413343720138073\n",
      "NLL Loss: 0.04691227898001671\n",
      "NLL Loss: 0.03382165729999542\n",
      "NLL Loss: 0.059308331459760666\n",
      "NLL Loss: 0.03417067602276802\n",
      "NLL Loss: 0.02507716417312622\n",
      "NLL Loss: 0.04303126409649849\n",
      "NLL Loss: 0.03809177502989769\n",
      "NLL Loss: 0.04091740772128105\n",
      "NLL Loss: 0.03870609030127525\n",
      "NLL Loss: 0.06748238950967789\n",
      "NLL Loss: 0.03395327925682068\n",
      "NLL Loss: 0.04681401327252388\n",
      "NLL Loss: 0.028330933302640915\n",
      "NLL Loss: 0.039650995284318924\n",
      "NLL Loss: 0.033006638288497925\n",
      "NLL Loss: 0.034047793596982956\n",
      "NLL Loss: 0.03251051902770996\n",
      "NLL Loss: 0.06245192140340805\n",
      "NLL Loss: 0.05099969729781151\n",
      "NLL Loss: 0.037602975964546204\n",
      "NLL Loss: 0.03528375178575516\n",
      "NLL Loss: 0.05450925976037979\n",
      "NLL Loss: 0.029818842187523842\n",
      "NLL Loss: 0.046588316559791565\n",
      "NLL Loss: 0.03888539969921112\n",
      "NLL Loss: 0.03697357699275017\n",
      "NLL Loss: 0.037806436419487\n",
      "NLL Loss: 0.03290238231420517\n",
      "NLL Loss: 0.03596772998571396\n",
      "NLL Loss: 0.04815124720335007\n",
      "NLL Loss: 0.032058730721473694\n",
      "NLL Loss: 0.05086436867713928\n",
      "NLL Loss: 0.026802022010087967\n",
      "NLL Loss: 0.04490595683455467\n",
      "NLL Loss: 0.04127582162618637\n",
      "NLL Loss: 0.02743701823055744\n",
      "NLL Loss: 0.038826558738946915\n",
      "NLL Loss: 0.039856918156147\n",
      "NLL Loss: 0.029312923550605774\n",
      "NLL Loss: 0.03885592892765999\n",
      "NLL Loss: 0.03651546314358711\n",
      "NLL Loss: 0.02849421463906765\n",
      "NLL Loss: 0.03572608157992363\n",
      "NLL Loss: 0.041477885097265244\n",
      "NLL Loss: 0.028369246050715446\n",
      "NLL Loss: 0.03924177959561348\n",
      "NLL Loss: 0.044013090431690216\n",
      "NLL Loss: 0.028534740209579468\n",
      "NLL Loss: 0.0387326180934906\n",
      "NLL Loss: 0.05037544295191765\n",
      "NLL Loss: 0.03201507031917572\n",
      "NLL Loss: 0.033461712300777435\n",
      "NLL Loss: 0.024002933874726295\n",
      "NLL Loss: 0.03476376831531525\n",
      "NLL Loss: 0.03193163499236107\n",
      "NLL Loss: 0.019689245149493217\n",
      "NLL Loss: 0.04255307838320732\n",
      "NLL Loss: 0.034988950937986374\n",
      "NLL Loss: 0.03166333585977554\n",
      "NLL Loss: 0.038149796426296234\n",
      "NLL Loss: 0.0363452285528183\n",
      "NLL Loss: 0.0315333791077137\n",
      "NLL Loss: 0.01980527862906456\n",
      "NLL Loss: 0.03762393444776535\n",
      "NLL Loss: 0.03978320211172104\n",
      "NLL Loss: 0.04918938875198364\n",
      "NLL Loss: 0.041381996124982834\n",
      "NLL Loss: 0.036582570523023605\n",
      "NLL Loss: 0.030146833509206772\n",
      "NLL Loss: 0.034489136189222336\n",
      "NLL Loss: 0.02435612678527832\n",
      "NLL Loss: 0.05142837017774582\n",
      "NLL Loss: 0.03166991099715233\n",
      "NLL Loss: 0.030254656448960304\n",
      "NLL Loss: 0.039013803005218506\n",
      "NLL Loss: 0.033578772097826004\n",
      "NLL Loss: 0.02775300294160843\n",
      "NLL Loss: 0.034434299916028976\n",
      "NLL Loss: 0.03732912987470627\n",
      "NLL Loss: 0.05183692276477814\n",
      "NLL Loss: 0.03126736357808113\n",
      "NLL Loss: 0.045160625129938126\n",
      "NLL Loss: 0.03239241614937782\n",
      "NLL Loss: 0.0216362327337265\n",
      "NLL Loss: 0.03976006805896759\n",
      "NLL Loss: 0.03894256427884102\n",
      "NLL Loss: 0.03128672391176224\n",
      "NLL Loss: 0.0408795103430748\n",
      "NLL Loss: 0.0265659186989069\n",
      "NLL Loss: 0.024801943451166153\n",
      "NLL Loss: 0.03423367813229561\n",
      "NLL Loss: 0.028468165546655655\n",
      "NLL Loss: 0.03856852278113365\n",
      "NLL Loss: 0.030390450730919838\n",
      "NLL Loss: 0.037288811057806015\n",
      "NLL Loss: 0.031737323850393295\n",
      "NLL Loss: 0.02785639837384224\n",
      "NLL Loss: 0.02766195498406887\n",
      "NLL Loss: 0.02900988794863224\n",
      "NLL Loss: 0.03641311451792717\n",
      "NLL Loss: 0.038473110646009445\n",
      "NLL Loss: 0.024324318394064903\n",
      "NLL Loss: 0.03148702532052994\n",
      "NLL Loss: 0.035850703716278076\n",
      "NLL Loss: 0.030903039500117302\n",
      "NLL Loss: 0.03984455764293671\n",
      "NLL Loss: 0.026526423171162605\n",
      "NLL Loss: 0.043502699583768845\n",
      "NLL Loss: 0.016024528071284294\n",
      "NLL Loss: 0.034279417246580124\n",
      "NLL Loss: 0.01724160462617874\n",
      "NLL Loss: 0.017626719549298286\n",
      "NLL Loss: 0.039838362485170364\n",
      "NLL Loss: 0.02890978939831257\n",
      "NLL Loss: 0.025105422362685204\n",
      "NLL Loss: 0.02118631638586521\n",
      "NLL Loss: 0.03340424224734306\n",
      "NLL Loss: 0.022249825298786163\n",
      "NLL Loss: 0.027737021446228027\n",
      "NLL Loss: 0.044326357543468475\n",
      "NLL Loss: 0.027914823964238167\n",
      "NLL Loss: 0.02979709766805172\n",
      "NLL Loss: 0.026851216331124306\n",
      "NLL Loss: 0.030476640909910202\n",
      "NLL Loss: 0.054509662091732025\n",
      "NLL Loss: 0.017436617985367775\n",
      "NLL Loss: 0.029177937656641006\n",
      "NLL Loss: 0.022238221019506454\n",
      "NLL Loss: 0.030157409608364105\n",
      "NLL Loss: 0.03293513134121895\n",
      "NLL Loss: 0.02550148032605648\n",
      "NLL Loss: 0.024137357249855995\n",
      "NLL Loss: 0.026310015469789505\n",
      "NLL Loss: 0.032754477113485336\n",
      "NLL Loss: 0.023365503177046776\n",
      "NLL Loss: 0.024080215021967888\n",
      "NLL Loss: 0.045441046357154846\n",
      "NLL Loss: 0.041288673877716064\n",
      "NLL Loss: 0.025947919115424156\n",
      "NLL Loss: 0.032005149871110916\n",
      "NLL Loss: 0.02336641028523445\n",
      "NLL Loss: 0.03102298080921173\n",
      "NLL Loss: 0.028796566650271416\n",
      "NLL Loss: 0.02900700829923153\n",
      "NLL Loss: 0.04772071912884712\n",
      "NLL Loss: 0.0252190250903368\n",
      "NLL Loss: 0.03976637125015259\n",
      "NLL Loss: 0.031245095655322075\n",
      "NLL Loss: 0.026698505505919456\n",
      "NLL Loss: 0.026564735919237137\n",
      "NLL Loss: 0.021197054535150528\n",
      "NLL Loss: 0.0449165515601635\n",
      "NLL Loss: 0.0225515253841877\n",
      "NLL Loss: 0.02964271418750286\n",
      "NLL Loss: 0.02091633342206478\n",
      "NLL Loss: 0.03303839638829231\n",
      "NLL Loss: 0.033184632658958435\n",
      "NLL Loss: 0.019903913140296936\n",
      "NLL Loss: 0.023492762818932533\n",
      "NLL Loss: 0.03180009126663208\n",
      "NLL Loss: 0.018528971821069717\n",
      "NLL Loss: 0.022482680156826973\n",
      "NLL Loss: 0.027389515191316605\n",
      "NLL Loss: 0.030940821394324303\n",
      "NLL Loss: 0.021581828594207764\n",
      "NLL Loss: 0.015551195479929447\n",
      "NLL Loss: 0.027423178777098656\n",
      "NLL Loss: 0.025917023420333862\n",
      "NLL Loss: 0.027436064556241035\n",
      "NLL Loss: 0.02349206991493702\n",
      "NLL Loss: 0.03136463090777397\n",
      "NLL Loss: 0.04962197318673134\n",
      "NLL Loss: 0.03310338407754898\n",
      "NLL Loss: 0.03220764175057411\n",
      "NLL Loss: 0.025138065218925476\n",
      "NLL Loss: 0.023844456300139427\n",
      "NLL Loss: 0.022329336032271385\n",
      "NLL Loss: 0.034946147352457047\n",
      "NLL Loss: 0.018866032361984253\n",
      "NLL Loss: 0.015233290381729603\n",
      "NLL Loss: 0.015236367471516132\n",
      "NLL Loss: 0.030495911836624146\n",
      "NLL Loss: 0.030703479424118996\n",
      "NLL Loss: 0.048442453145980835\n",
      "NLL Loss: 0.021650133654475212\n",
      "NLL Loss: 0.025572603568434715\n",
      "NLL Loss: 0.029408631846308708\n",
      "NLL Loss: 0.026267236098647118\n",
      "NLL Loss: 0.015160770155489445\n",
      "NLL Loss: 0.021158764138817787\n",
      "NLL Loss: 0.01775245927274227\n",
      "NLL Loss: 0.024390924721956253\n",
      "NLL Loss: 0.02349737286567688\n",
      "NLL Loss: 0.01726810447871685\n",
      "NLL Loss: 0.024010736495256424\n",
      "NLL Loss: 0.02889763005077839\n",
      "NLL Loss: 0.01705285906791687\n",
      "NLL Loss: 0.028156325221061707\n",
      "NLL Loss: 0.0229878518730402\n",
      "NLL Loss: 0.03199412673711777\n",
      "NLL Loss: 0.030250027775764465\n",
      "NLL Loss: 0.025403905659914017\n",
      "NLL Loss: 0.024096697568893433\n",
      "NLL Loss: 0.014077063649892807\n",
      "NLL Loss: 0.021412836387753487\n",
      "NLL Loss: 0.03133481740951538\n",
      "NLL Loss: 0.025724364444613457\n",
      "NLL Loss: 0.022304223850369453\n",
      "NLL Loss: 0.019758494570851326\n",
      "NLL Loss: 0.022761350497603416\n",
      "NLL Loss: 0.024878332391381264\n",
      "NLL Loss: 0.022234775125980377\n",
      "NLL Loss: 0.023964673280715942\n",
      "NLL Loss: 0.029396139085292816\n",
      "NLL Loss: 0.02266986109316349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL Loss: 0.030006807297468185\n",
      "NLL Loss: 0.03098428249359131\n",
      "NLL Loss: 0.023853838443756104\n",
      "NLL Loss: 0.017495010048151016\n",
      "NLL Loss: 0.02384697087109089\n",
      "NLL Loss: 0.02303258329629898\n",
      "NLL Loss: 0.012917105108499527\n",
      "NLL Loss: 0.020390067249536514\n",
      "NLL Loss: 0.023172516375780106\n",
      "NLL Loss: 0.024942034855484962\n",
      "NLL Loss: 0.02567211724817753\n",
      "NLL Loss: 0.03368431702256203\n",
      "NLL Loss: 0.02081015706062317\n",
      "NLL Loss: 0.02715565823018551\n",
      "NLL Loss: 0.02164369821548462\n",
      "NLL Loss: 0.027976712211966515\n",
      "NLL Loss: 0.03575754165649414\n",
      "NLL Loss: 0.04442604258656502\n",
      "NLL Loss: 0.018933819606900215\n",
      "NLL Loss: 0.029846396297216415\n",
      "NLL Loss: 0.02199404314160347\n",
      "NLL Loss: 0.02686881832778454\n",
      "NLL Loss: 0.0423690490424633\n",
      "NLL Loss: 0.01739085279405117\n",
      "NLL Loss: 0.02086290717124939\n",
      "NLL Loss: 0.020008353516459465\n",
      "NLL Loss: 0.021676957607269287\n",
      "NLL Loss: 0.02381288632750511\n",
      "NLL Loss: 0.021141819655895233\n",
      "NLL Loss: 0.029934167861938477\n",
      "NLL Loss: 0.015182336792349815\n",
      "NLL Loss: 0.0244023147970438\n",
      "NLL Loss: 0.02285771258175373\n",
      "NLL Loss: 0.024783262982964516\n",
      "NLL Loss: 0.02212299220263958\n",
      "NLL Loss: 0.02983398362994194\n",
      "NLL Loss: 0.013443658128380775\n",
      "NLL Loss: 0.02256856858730316\n",
      "NLL Loss: 0.03718390688300133\n",
      "NLL Loss: 0.014339664950966835\n",
      "NLL Loss: 0.027908897027373314\n",
      "NLL Loss: 0.015768898651003838\n",
      "NLL Loss: 0.016276175156235695\n",
      "NLL Loss: 0.023055167868733406\n",
      "NLL Loss: 0.019041307270526886\n",
      "NLL Loss: 0.025203293189406395\n",
      "NLL Loss: 0.02479267120361328\n",
      "NLL Loss: 0.02752998098731041\n",
      "NLL Loss: 0.014283351600170135\n",
      "NLL Loss: 0.036255672574043274\n",
      "NLL Loss: 0.011861306615173817\n",
      "NLL Loss: 0.020678119733929634\n",
      "NLL Loss: 0.01979648694396019\n",
      "NLL Loss: 0.018694153055548668\n",
      "NLL Loss: 0.022561637684702873\n",
      "NLL Loss: 0.01050123106688261\n",
      "NLL Loss: 0.01821577548980713\n",
      "NLL Loss: 0.016507713124155998\n",
      "NLL Loss: 0.019940366968512535\n",
      "NLL Loss: 0.022849610075354576\n",
      "NLL Loss: 0.022805746644735336\n",
      "NLL Loss: 0.01719040237367153\n",
      "NLL Loss: 0.02650575339794159\n",
      "NLL Loss: 0.02221929468214512\n",
      "NLL Loss: 0.013705099001526833\n",
      "NLL Loss: 0.030013469979166985\n",
      "NLL Loss: 0.020632650703191757\n",
      "NLL Loss: 0.030046220868825912\n",
      "NLL Loss: 0.02024725079536438\n",
      "NLL Loss: 0.027207210659980774\n",
      "NLL Loss: 0.026495903730392456\n",
      "NLL Loss: 0.019075648859143257\n",
      "NLL Loss: 0.023788126185536385\n",
      "NLL Loss: 0.019888533279299736\n",
      "NLL Loss: 0.030417582020163536\n",
      "NLL Loss: 0.027751224115490913\n",
      "NLL Loss: 0.014873058535158634\n",
      "NLL Loss: 0.0341709740459919\n",
      "NLL Loss: 0.0200825035572052\n",
      "NLL Loss: 0.019987262785434723\n",
      "NLL Loss: 0.020360806956887245\n",
      "NLL Loss: 0.020196931436657906\n",
      "NLL Loss: 0.030060121789574623\n",
      "NLL Loss: 0.016839193180203438\n",
      "NLL Loss: 0.027018846943974495\n",
      "NLL Loss: 0.014575602486729622\n",
      "NLL Loss: 0.019827203825116158\n",
      "NLL Loss: 0.023000042885541916\n",
      "NLL Loss: 0.04524543881416321\n",
      "NLL Loss: 0.025059957057237625\n",
      "NLL Loss: 0.02078252099454403\n",
      "NLL Loss: 0.028703484684228897\n",
      "NLL Loss: 0.01679886132478714\n",
      "NLL Loss: 0.019679151475429535\n",
      "NLL Loss: 0.03790369629859924\n",
      "NLL Loss: 0.02337777242064476\n",
      "NLL Loss: 0.031423766165971756\n",
      "NLL Loss: 0.024043111130595207\n",
      "NLL Loss: 0.022225597873330116\n",
      "NLL Loss: 0.023492339998483658\n",
      "NLL Loss: 0.027676712721586227\n",
      "NLL Loss: 0.03691437840461731\n",
      "NLL Loss: 0.024928579106926918\n",
      "NLL Loss: 0.014567509293556213\n",
      "NLL Loss: 0.02407757192850113\n",
      "NLL Loss: 0.0108754588291049\n",
      "NLL Loss: 0.019865505397319794\n",
      "NLL Loss: 0.022692477330565453\n",
      "NLL Loss: 0.027007510885596275\n",
      "NLL Loss: 0.02025461569428444\n",
      "NLL Loss: 0.01825757883489132\n",
      "NLL Loss: 0.02015404775738716\n",
      "NLL Loss: 0.027979403734207153\n",
      "NLL Loss: 0.013914202339947224\n",
      "NLL Loss: 0.032679084688425064\n",
      "NLL Loss: 0.009266771376132965\n",
      "NLL Loss: 0.032290488481521606\n",
      "NLL Loss: 0.04164531081914902\n",
      "NLL Loss: 0.01771642081439495\n",
      "NLL Loss: 0.025341186672449112\n",
      "NLL Loss: 0.026473280042409897\n",
      "NLL Loss: 0.015712765976786613\n",
      "NLL Loss: 0.029795674607157707\n",
      "NLL Loss: 0.02714383602142334\n",
      "NLL Loss: 0.027971705421805382\n",
      "NLL Loss: 0.016949187964200974\n",
      "NLL Loss: 0.02368113026022911\n",
      "NLL Loss: 0.027250070124864578\n",
      "NLL Loss: 0.012051205150783062\n",
      "NLL Loss: 0.03249620273709297\n",
      "NLL Loss: 0.01680990308523178\n",
      "NLL Loss: 0.020159104838967323\n",
      "NLL Loss: 0.024985304102301598\n",
      "NLL Loss: 0.03001588024199009\n",
      "NLL Loss: 0.03112756833434105\n",
      "NLL Loss: 0.016248222440481186\n",
      "NLL Loss: 0.016832320019602776\n",
      "NLL Loss: 0.023136816918849945\n",
      "NLL Loss: 0.03468253090977669\n",
      "NLL Loss: 0.025946931913495064\n",
      "NLL Loss: 0.019101303070783615\n",
      "NLL Loss: 0.02784874476492405\n",
      "NLL Loss: 0.02663925476372242\n",
      "NLL Loss: 0.016555294394493103\n",
      "NLL Loss: 0.024366924539208412\n",
      "NLL Loss: 0.02293187379837036\n",
      "NLL Loss: 0.020898330956697464\n",
      "NLL Loss: 0.03061763197183609\n",
      "NLL Loss: 0.013522271998226643\n",
      "NLL Loss: 0.026143446564674377\n",
      "NLL Loss: 0.02985607273876667\n",
      "NLL Loss: 0.01761336252093315\n",
      "NLL Loss: 0.022311829030513763\n",
      "NLL Loss: 0.018709532916545868\n",
      "NLL Loss: 0.027095820754766464\n",
      "NLL Loss: 0.01621643453836441\n",
      "NLL Loss: 0.03281019255518913\n",
      "NLL Loss: 0.027517084032297134\n",
      "NLL Loss: 0.015854177996516228\n",
      "NLL Loss: 0.020964473485946655\n",
      "NLL Loss: 0.02054087072610855\n",
      "NLL Loss: 0.02158285118639469\n",
      "NLL Loss: 0.016809191554784775\n",
      "NLL Loss: 0.020394491031765938\n",
      "NLL Loss: 0.014178969897329807\n",
      "NLL Loss: 0.019916828721761703\n",
      "NLL Loss: 0.020385928452014923\n",
      "NLL Loss: 0.020506015047430992\n",
      "NLL Loss: 0.024722062051296234\n",
      "NLL Loss: 0.018252389505505562\n",
      "NLL Loss: 0.01455315388739109\n",
      "NLL Loss: 0.032662346959114075\n",
      "NLL Loss: 0.017751464620232582\n",
      "NLL Loss: 0.027084724977612495\n",
      "NLL Loss: 0.023868024349212646\n",
      "NLL Loss: 0.01971682719886303\n",
      "NLL Loss: 0.01232924684882164\n",
      "NLL Loss: 0.013589594513177872\n",
      "NLL Loss: 0.019072400406003\n",
      "NLL Loss: 0.017315885052084923\n",
      "NLL Loss: 0.015408224426209927\n",
      "NLL Loss: 0.016108103096485138\n",
      "NLL Loss: 0.020920226350426674\n",
      "NLL Loss: 0.01926262117922306\n",
      "NLL Loss: 0.02253374084830284\n",
      "NLL Loss: 0.03169681504368782\n",
      "NLL Loss: 0.02671843022108078\n",
      "NLL Loss: 0.00965193472802639\n",
      "NLL Loss: 0.009130755439400673\n",
      "NLL Loss: 0.021621428430080414\n",
      "NLL Loss: 0.01739666797220707\n",
      "NLL Loss: 0.017476271837949753\n",
      "NLL Loss: 0.018220432102680206\n",
      "NLL Loss: 0.03139720484614372\n",
      "NLL Loss: 0.015361853875219822\n",
      "NLL Loss: 0.029791317880153656\n",
      "NLL Loss: 0.016395725309848785\n",
      "NLL Loss: 0.020404843613505363\n",
      "NLL Loss: 0.019665870815515518\n",
      "NLL Loss: 0.022237112745642662\n",
      "NLL Loss: 0.019662506878376007\n",
      "NLL Loss: 0.014602256007492542\n",
      "NLL Loss: 0.033559612929821014\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN().to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    print(\"NLL Loss: {}\".format(loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7990\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
